{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on colab or locally\n",
    "try:\n",
    "\tfrom google.colab import files\n",
    "\tRUNNING_IN_COLAB = True\n",
    "\tprint(\"Running on Google Colab.\")\n",
    "except ModuleNotFoundError:\n",
    "\tRUNNING_IN_COLAB = False\n",
    "\tprint(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the git repository of the project for the source files\n",
    "!git clone https://github.com/valeriodiste/computer_vision_project_dev.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to the cloned repository\n",
    "# TO DO: change the directory to the correct one\n",
    "%cd /content/computer_vision_project_dev\n",
    "# Pull the latest changes from the repository\n",
    "!git pull origin main\n",
    "# Change the working directory to the parent directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "# %%capture\n",
    "%pip install pytorch-lightning\n",
    "%pip install pycocotools\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the standard libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Import the PyTorch libraries and modules\n",
    "import torch\n",
    "\n",
    "# Import the PyTorch Lightning libraries and modules\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import the coco library\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Import the W&B (Weights & Biases) library\n",
    "# import wandb\n",
    "# from wandb.sdk import wandb_run\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import cv2\n",
    "import base64\n",
    "\n",
    "# Import the tqdm library (for the progress bars)\n",
    "if not RUNNING_IN_COLAB:\n",
    "\tfrom tqdm import tqdm\n",
    "else:\n",
    "\tfrom tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom modules\n",
    "if not RUNNING_IN_COLAB:\n",
    "\t# We are running locally (not on Google Colab, import modules from the \"src\" directory in the current directory)\n",
    "\tfrom src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore\n",
    "else:\n",
    "\t# We are running on Google Colab (import modules from the pulled repository stored in the project's directory)\n",
    "\tfrom computer_vision_project_dev.src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom computer_vision_project_dev.src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "\n",
    "# ===== Training & Datasets constants =====================================================\n",
    "\n",
    "# MS COCO dataset constants (use MS COCO 2014 dataset for image captioning)\n",
    "COCO_DATA_YEAR = '2014'  \t# '2014' or '2017'\n",
    "COCO_DATA_TYPE = 'val'  # 'train' or 'val'\n",
    "COCO_DATA_CAPTIONS_FILE = f\"/annotations/captions_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the annotations file inside the DATA_FOLDER\n",
    "CODO_DATA_INSTANCES_FILE = f\"/annotations/instances_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the instances file inside the DATA_FOLDER\n",
    "\n",
    "# Size of the image patches\n",
    "IMAGE_PATCH_SIZE = 16\n",
    "# Number of image patches per dimension (i.e. both vertically and horizontally, since images have a square aspect ratio)\n",
    "IMAGE_PATCHES_PER_DIMENSION = 10\t# 3x3 patches, 48x48 pixels images\n",
    "\n",
    "# Total number of images to consider in the dataset (will be split into training, validation and test sets)\n",
    "NUMBER_OF_IMAGES_IN_DB = 10 \t# Was 2000\n",
    "# Minimum number of captions for an image\n",
    "MIN_IMAGE_CAPTIONS = 5\n",
    "# If not enough square images are found, also accept images that have this max aspect difference (they will be cropped to a square aspect ratio later)\n",
    "MAX_ASPECT_RATIO_TOLERANCE = 0.1 \t# Accept images that are 10% wider than they are tall (or vice versa)\n",
    "\n",
    "# ===== Evaluation constants ==============================================================\n",
    "\n",
    "# Define the number of images K to retrieve for each query and the number of queries N to calculate the mean average precision (MAP@K)\n",
    "MAP_K = 10\n",
    "MAP_N = 10\n",
    "\n",
    "# Define the number of images K to retrieve for each query to calculate the Recall@K metrics\n",
    "RECALL_K = 1_000\n",
    "\n",
    "# Whether to print the debug information during the MAP@K and Recall@K evaluation of the models\n",
    "PRINT_EVALUATION_DEBUG = True\n",
    "\n",
    "# Whether to evaluate the models (i.e. compute the MAP@K and Recall@K metrics for the trained models on the test datasets)\n",
    "EVALUATE_MODELS = True\n",
    "\n",
    "# ===== MAIN CONSTANTS =====================================================================\n",
    "\n",
    "# Define the data folder, onto which the various dictionaries, lists and other data will be saved\n",
    "DATA_FOLDER = \"src/data\" if not RUNNING_IN_COLAB else \"/content/data\"\n",
    "\n",
    "# Define the path to save models\n",
    "MODELS_FOLDER = \"src/models\" if not RUNNING_IN_COLAB else \"/content/models\"\n",
    "\n",
    "# Force the creation of the \"image_db\" images list, the JSON files for the datasets, ecc...\n",
    "FORCE_DICTIONARIES_CREATION = True\t\t# Set to false to try to load the dictionaries from the DATA_FOLDER if they exist\n",
    "\n",
    "# Whether to load model checkpoints (if they were already saved locally) or not\n",
    "LOAD_MODELS_CHECKPOINTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WANDB_API_KEY (set to \"\" to disable W&B logging)\n",
    "# NOTE: leaving the WANDB_API_KEY to a value of None will throw an error\n",
    "WANDB_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wandb logger, api object, entity name and project name\n",
    "wandb_logger = None\n",
    "wandb_api = None\n",
    "wandb_entity = None\n",
    "wandb_project = None\n",
    "# Check if a W&B api key is provided\n",
    "if WANDB_API_KEY == None:\n",
    "\tprint(\"No W&B API key provided, please provide a valid key to use the W&B API or set the WANDB_API_KEY variable to an empty string to disable logging\")\n",
    "\traise ValueError(\"No W&B API key provided...\")\n",
    "elif WANDB_API_KEY != \"\":\n",
    "\t# Login to the W&B (Weights & Biases) API\n",
    "\twandb.login(key=WANDB_API_KEY, relogin=True)\n",
    "\t# Minimize the logging from the W&B (Weights & Biases) library\n",
    "\tos.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\tlogging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "\t# Initialize the W&B (Weights & Biases) loggger\n",
    "\twandb_logger = WandbLogger(\n",
    "\t\tlog_model=\"all\", project=\"cv-dsi-project\", name=\"- SEPARATOR -\")\n",
    "\t# Initialize the W&B (Weights & Biases) API\n",
    "\twandb_api = wandb.Api()\n",
    "\t# Get the W&B (Weights & Biases) entity name\n",
    "\twandb_entity = wandb_logger.experiment.entity\n",
    "\t# Get the W&B (Weights & Biases) project name\n",
    "\twandb_project = wandb_logger.experiment.project\n",
    "\t# Finish the \"separator\" experiment\n",
    "\twandb_logger.experiment.finish(quiet=True)\n",
    "\tprint(\"W&B API key provided, logging with W&B enabled.\")\n",
    "else:\n",
    "\tprint(\"No W&B API key provided, logging with W&B disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they do not exist\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "\tprint(f\"Creating the data folder at '{DATA_FOLDER}'...\")\n",
    "\tos.makedirs(DATA_FOLDER)\n",
    "if not os.path.exists(MODELS_FOLDER):\n",
    "\tprint(f\"Creating the models folder at '{MODELS_FOLDER}'...\")\n",
    "\tos.makedirs(MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the annotation file for the COCO dataset exists, if it does not exist, download it\n",
    "!cd {DATA_FOLDER} && wget -nc http://images.cocodataset.org/annotations/annotations_trainval{COCO_DATA_YEAR}.zip\n",
    "!cd {DATA_FOLDER} && unzip -n annotations_trainval{COCO_DATA_YEAR}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the COCO api for captioning\n",
    "coco_captions = COCO(f\"{DATA_FOLDER}{COCO_DATA_CAPTIONS_FILE}\")\n",
    "# Initialize the COCO api for object detection\n",
    "coco_instances = COCO(f\"{DATA_FOLDER}{CODO_DATA_INSTANCES_FILE}\")\n",
    "\n",
    "# Show the COCO dataset info for the captioning task\n",
    "print(\"\\nCOCO captioning dataset infos:\")\n",
    "coco_captions.info()\n",
    "\n",
    "# Show the information for the captioning task\n",
    "print(\"\\nCOCO captioning task infos:\")\n",
    "coco_caps = coco_captions.dataset['annotations']\n",
    "print(\"Number of images: \", len(coco_captions.getImgIds()))\n",
    "print(\"Number of captions: \", len(coco_caps))\n",
    "print(\"Number of average captions per image: \", len(coco_caps) / len(coco_captions.getImgIds()))\n",
    "\n",
    "# Show the COCO dataset info for the object detection task\n",
    "print(\"\\nCOCO object detection dataset infos:\")\n",
    "coco_instances.info()\n",
    "\n",
    "# Show the information for the object detection task\n",
    "print(\"\\nCOCO object detection task infos:\")\n",
    "coco_objs = coco_instances.dataset['annotations']\n",
    "print(\"Number of images: \", len(coco_instances.getImgIds()))\n",
    "print(\"Number of objects: \", len(coco_objs))\n",
    "print(\"Number of categories: \", len(coco_instances.cats))\n",
    "print(\"Categories:\")\n",
    "utils.print_json(coco_instances.cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some examples from the MS COCO dataset\n",
    "\n",
    "# Print the first image object example\n",
    "example_image_index = 0\n",
    "print(\"\\nImage object example: \")\n",
    "image_example = coco_captions.loadImgs(coco_caps[example_image_index]['image_id'])[0]\n",
    "utils.print_json(image_example, 2)\n",
    "\n",
    "# Print the actual image file\n",
    "print(\"\\nActual image of the example (size: \" + str(image_example['width']) + \"x\" + str(image_example['height']) + \"):\")\n",
    "url = image_example['coco_url']\n",
    "image = io.imread(url)\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Downscale the image to the maximum allowed size in the model\n",
    "image_max_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "# Crop the image to a square aspect ratio if it is not already square\n",
    "downscaled_image = image\n",
    "if image_example['width'] > image_example['height']:\n",
    "\t# Image is wider than tall, crop the sides\n",
    "\tcrop_width = (image_example['width'] - image_example['height']) // 2\n",
    "\tdownscaled_image = image[:, crop_width:crop_width+image_example['height']]\n",
    "elif image_example['height'] > image_example['width']:\n",
    "\t# Image is taller than wide, crop the top and bottom\n",
    "\tcrop_height = (image_example['height'] - image_example['width']) // 2\n",
    "\tdownscaled_image = image[crop_height:crop_height+image_example['width'], :]\n",
    "# Downscale the image to the maximum allowed size\n",
    "downscaled_image = cv2.resize(downscaled_image, (image_max_size, image_max_size))\n",
    "print(\"\\nDownscaled & cropped image of the example (size: \" + str(image_max_size) + \"x\" + str(image_max_size) + \"):\")\n",
    "plt.axis('off')\n",
    "plt.imshow(downscaled_image)\n",
    "plt.show()\n",
    "\n",
    "# Print the captions for the given image\n",
    "print(\"\\nCaption examples for the given image: \")\n",
    "captions_for_image = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=image_example['id']))\n",
    "for caption, i in zip(captions_for_image, range(len(captions_for_image))):\n",
    "\tprint(str(i+1) + \") \" + caption['caption'].strip())\n",
    "\n",
    "# Print the captioning object example\n",
    "print(\"\\nFirst caption object example:\")\n",
    "utils.print_json(captions_for_image[0], 2)\n",
    "\n",
    "# Print information about the object detection task for the given image\n",
    "print(\"\\nObject detection examples for the given image:\")\n",
    "# Get the object detection annotations for the given image\n",
    "annotations_for_image = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=image_example['id']))\n",
    "print(\"List of the \" + str(len(annotations_for_image)) + \" object detection annotations for the given image (obtained using the 'coco_instances.loadAnns(image_annotation_id)' function):\")\n",
    "for annotation, i in zip(annotations_for_image, range(len(annotations_for_image))):\n",
    "\tprint(\"\\n> Annotation \" + str(i+1) + \":\")\n",
    "\t# Truncate the \"segmentation\" field if it is too long\n",
    "\ttruncation_length = 10\n",
    "\tfor j in range(len(annotation['segmentation'])):\n",
    "\t\tif len(annotation['segmentation'][j]) > truncation_length:\n",
    "\t\t\tannotation['segmentation'][j] = annotation['segmentation'][j][:truncation_length] + [\"...\"] + [\"[truncated to \" + str(truncation_length) + \" out of \" + str(len(annotation['segmentation'][j])) + \" elements]\"]\n",
    "\t\tbreak\n",
    "\t# Print the annotation object\n",
    "\tutils.print_json(annotation, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset of images for the training of the Vision Transformer model\n",
    "\n",
    "# Function that returns the list containing the images for the training of the Vision Transformer model\n",
    "def get_images_db(number_of_images):\n",
    "\t# Structure of the images\n",
    "\timages_list_object = {\n",
    "\t\t\"image_id\": \"\",\t\t\t# ID of the image (as found in the COCO dataset)\n",
    "\t\t\"image_url\": \"\",\t\t# URL of the image\n",
    "\t\t\"image_width\": 0,\t\t# The original image width\n",
    "\t\t\"image_height\": 0,\t\t# The original image height\n",
    "\t\t\"image_captions\": [],\t# List of captions for the image\n",
    "\t\t\"image_classes\": [\t\t# List of classes for the image (i.e. detected objects, in the order of area size)\n",
    "\t\t\t{\n",
    "\t\t\t\t\"class_id\": 0,\t\t# ID of the class (as found in the COCO dataset)\n",
    "\t\t\t\t\"class_name\": \"\",\t# Name of the class\n",
    "\t\t\t\t\"class_area\": 0,\t\t# Area of the class in the image\n",
    "\t\t\t\t\"class_bounding_box\": [0, 0, 0, 0]\t# Bounding box of the class in the format: \"[x, y, width, height]\" (normalized to the image size)\n",
    "\t\t\t}\n",
    "\t\t],\t\n",
    "\t\t\"image_data\": \"\"\t\t# Base64 string of the image\n",
    "\t}\n",
    "\t# Get the image ids\n",
    "\timg_ids = coco_captions.getImgIds()\n",
    "\t# Randomly shuffle the image ids\n",
    "\t# random.shuffle(img_ids)\n",
    "\t# Get the images\n",
    "\timages = []\n",
    "\t# Function that returns an image's base64 string from the image url\n",
    "\tdef get_image_data(image_url):\n",
    "\t\t# Load the image\n",
    "\t\timage = io.imread(image_url)\n",
    "\t\t# Downscale the image to the maximum allowed size in the model\n",
    "\t\timage_max_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "\t\t# Crop the image to a square aspect ratio if it is not already square\n",
    "\t\tdownscaled_image = image\n",
    "\t\tif image.shape[1] > image.shape[0]:\n",
    "\t\t\t# Image is wider than tall, crop the sides\n",
    "\t\t\tcrop_width = (image.shape[1] - image.shape[0]) // 2\n",
    "\t\t\tdownscaled_image = image[:, crop_width:crop_width+image.shape[0]]\n",
    "\t\telif image.shape[0] > image.shape[1]:\n",
    "\t\t\t# Image is taller than wide, crop the top and bottom\n",
    "\t\t\tcrop_height = (image.shape[0] - image.shape[1]) // 2\n",
    "\t\t\tdownscaled_image = image[crop_height:crop_height+image.shape[1], :]\n",
    "\t\t# Downscale the image to the maximum allowed size\n",
    "\t\tdownscaled_image = cv2.resize(downscaled_image, (image_max_size, image_max_size))\n",
    "\t\t# Convert the image to a base64 string\n",
    "\t\timage_base64 = base64.b64encode(cv2.imencode('.jpg', downscaled_image)[1]).decode()\n",
    "\t\t# Return the base64 string of the image\n",
    "\t\treturn image_base64\n",
    "\t# Function that returns a list of images with the given aspect ratio tolerance\n",
    "\tdef select_images_list(image_aspect_ratio_tolerance):\n",
    "\t\t# Get the images\n",
    "\t\tfor img_id in img_ids:\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg_obj = coco_captions.loadImgs(img_id)[0]\n",
    "\t\t\t# Check if the size of the image is square or within the aspect ratio tolerance\n",
    "\t\t\timage_aspect_ratio = img_obj['width'] / img_obj['height']\n",
    "\t\t\tif abs(image_aspect_ratio - 1) > image_aspect_ratio_tolerance:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Check if the image is already in the images list\n",
    "\t\t\tif any(img['image_id'] == img_obj['id'] for img in images):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Get the image url\n",
    "\t\t\timg_url = img_obj['coco_url']\n",
    "\t\t\t# Get the captions for the image\n",
    "\t\t\timg_captions = []\n",
    "\t\t\tcaptions = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tfor caption in captions:\n",
    "\t\t\t\tcaption_text = caption['caption'].strip()\n",
    "\t\t\t\tif len(caption_text) > 1:\n",
    "\t\t\t\t\timg_captions.append(caption_text)\n",
    "\t\t\t# Discard the image if the number of captions is less than the minimum\n",
    "\t\t\tif len(img_captions) < MIN_IMAGE_CAPTIONS:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Discard the image if it has no classes\n",
    "\t\t\tclasses = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tif len(classes) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Create a classes object with the fields: \"class_id\", \"class_name\", \"class_area\"\n",
    "\t\t\tclasses_obj = []\n",
    "\t\t\tfor class_obj in classes:\n",
    "\t\t\t\tclasses_obj.append({\n",
    "\t\t\t\t\t\"class_id\": class_obj['category_id'],\n",
    "\t\t\t\t\t\"class_name\": coco_instances.cats[class_obj['category_id']]['name'],\n",
    "\t\t\t\t\t\"class_area\": class_obj['area'],\n",
    "\t\t\t\t\t\"class_bounding_box\": class_obj['bbox']\n",
    "\t\t\t\t})\n",
    "\t\t\t# Sort the classes by area size\n",
    "\t\t\tclasses_obj = sorted(classes_obj, key=lambda x: x['class_area'], reverse=True)\n",
    "\t\t\t# Add the image to the images list\n",
    "\t\t\timages_list_object = {\n",
    "\t\t\t\t\"image_id\": img_obj['id'],\n",
    "\t\t\t\t\"image_url\": img_url,\n",
    "\t\t\t\t\"image_width\": img_obj['width'],\n",
    "\t\t\t\t\"image_height\": img_obj['height'],\n",
    "\t\t\t\t\"image_captions\": img_captions,\n",
    "\t\t\t\t\"image_classes\": classes_obj,\n",
    "\t\t\t\t\"image_data\": None # Will be filled later\n",
    "\t\t\t}\n",
    "\t\t\timages.append(images_list_object)\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif len(images) >= number_of_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Return the images list\n",
    "\t\treturn images\n",
    "\tprint(\"Selecting images with a square aspect ratio...\")\n",
    "\t# Get the images that have a square aspect ratio first\n",
    "\timages = select_images_list(0)\n",
    "\t# Get the remaining images with the given aspect ratio tolerance\n",
    "\tif len(images) < number_of_images:\n",
    "\t\tsquare_aspect_ratio_images = len(images)\n",
    "\t\tprint(\"> Found \" + str(square_aspect_ratio_images) + \" / \" + str(number_of_images) + \" images with a square aspect ratio, looking for the remaining images...\")\n",
    "\t\tprint(\"Looking for remaining images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"% (either a \" + str(1 + MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio or a \" + str(1 - MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio)...\")\n",
    "\t\timages = select_images_list(MAX_ASPECT_RATIO_TOLERANCE)\n",
    "\t\tnon_square_aspect_ratio_images = len(images) - square_aspect_ratio_images\n",
    "\t\t# Print the number of images found\n",
    "\t\tprint(\"> Found \" + str(non_square_aspect_ratio_images) + \" / \" + str(number_of_images) + \" more images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"%.\")\n",
    "\telse:\n",
    "\t\tprint(\"> Found \" + str(number_of_images) + \" / \" + str(number_of_images) + \" images with a square aspect ratio.\")\n",
    "\t# Print a message based on the number of images found\n",
    "\tif len(images) < number_of_images:\n",
    "\t\tprint(\"WARNING: Could not find enough images with the required aspect ratio tolerance, only \" + str(len(images)) + \" / \" + str(number_of_images) + \" images found.\")\n",
    "\telse:\n",
    "\t\tprint(\"DONE: Found all \" + str(number_of_images) + \" / \" + str(number_of_images) + \" images with the required aspect ratio tolerance.\")\n",
    "\t# Get all the image data\n",
    "\tfor img in tqdm(images, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\timg[\"image_data\"] = get_image_data(img['image_url'])\n",
    "\t# Return the images list\n",
    "\treturn images\n",
    "\n",
    "# List of image objects used for the training of the Vision Transformer model\n",
    "images_db = []\n",
    "\n",
    "# Check if the images list should be rebuilt or loaded\n",
    "create_images_db = True\n",
    "images_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "if os.path.exists(images_db_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\twith open(images_db_file, 'r') as f:\n",
    "\t\timages_db = json.load(f)\n",
    "\tif len(images_db) == NUMBER_OF_IMAGES_IN_DB:\n",
    "\t\tcreate_images_db = False\n",
    "\t\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "if create_images_db or FORCE_DICTIONARIES_CREATION:\n",
    "\t# Initialize the images list\n",
    "\timages_db = get_images_db(NUMBER_OF_IMAGES_IN_DB)\n",
    "\t# Save the images list to a JSON file\n",
    "\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\tprint(\"Saving the images list to the file: \", images_db_file)\n",
    "\twith open(images_db_file, 'w') as f:\n",
    "\t\tjson.dump(images_db, f)\n",
    "\n",
    "# Print the final number of images in the dataset\n",
    "print(\"\\nNumber of loaded images in the dataset: \" + str(len(images_db)) + \"/\" + str(NUMBER_OF_IMAGES_IN_DB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first image object example\n",
    "example_image_index = -1\n",
    "print(\"Image object example: \")\n",
    "utils.print_json(images_db[example_image_index], 2)\n",
    "\n",
    "# Print the actual image file\n",
    "image = utils.get_image_from_db_object(images_db[example_image_index])\n",
    "print(\"\\nActual image of the example (original size: \" + str(images_db[example_image_index]['image_width']) + \"x\" + str(images_db[example_image_index]['image_height']) + \" | downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Print how the Transformer model sees the image\n",
    "print(\"\\nHow the Transformer model sees the image (downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "# Divide the image into smaller images representing the patches\n",
    "image_patches = []\n",
    "for i in range(0, image.shape[0], IMAGE_PATCH_SIZE):\n",
    "\tfor j in range(0, image.shape[1], IMAGE_PATCH_SIZE):\n",
    "\t\timage_patch = image[i:i+IMAGE_PATCH_SIZE, j:j+IMAGE_PATCH_SIZE]\n",
    "\t\timage_patches.append(image_patch)\n",
    "# Display the image patches\n",
    "fig, axs = plt.subplots(IMAGE_PATCHES_PER_DIMENSION, IMAGE_PATCHES_PER_DIMENSION, figsize=(10, 10))\n",
    "for i in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\tfor j in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\t\taxs[i, j].imshow(image_patches[i*IMAGE_PATCHES_PER_DIMENSION+j])\n",
    "\t\taxs[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the max length of the image IDS (we consider the index of the image in the \"images_db\" as the image ID)\n",
    "max_image_id_length = len(images_db)\n",
    "\n",
    "# Number of output tokens for the encoded image IDs (the 10 digits [0-9] plus the 3 special tokens, i.e. end of sequence, padding, start of sequence)\n",
    "output_tokens = 10 + 3\n",
    "\n",
    "# Build the Transformer Indexing Database for training the vision transformer\n",
    "transformer_indexing_dataset = datasets.TransformerIndexingDataset(\n",
    "\timages=images_db,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=IMAGE_PATCHES_PER_DIMENSION,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=os.path.join(DATA_FOLDER, \"transformer_indexing_dataset.json\"),\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION\n",
    ")\n",
    "\n",
    "# Print the first example from the Transformer Indexing Dataset\n",
    "example_index = 0\n",
    "print(\"Example from the Transformer Indexing Dataset:\")\n",
    "print(\"<encoded_image, encoded_image_id> tuple:\")\n",
    "print(transformer_indexing_dataset[example_index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
