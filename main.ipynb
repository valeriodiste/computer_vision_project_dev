{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on colab or locally\n",
    "try:\n",
    "\tfrom google.colab import files\n",
    "\tRUNNING_IN_COLAB = True\n",
    "\tprint(\"Running on Google Colab.\")\n",
    "except ModuleNotFoundError:\n",
    "\tRUNNING_IN_COLAB = False\n",
    "\tprint(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the git repository of the project for the source files\n",
    "!git clone https://github.com/valeriodiste/computer_vision_project_dev.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to the cloned repository\n",
    "# TO DO: change the directory to the correct one\n",
    "%cd /content/computer_vision_project_dev\n",
    "# Pull the latest changes from the repository\n",
    "!git pull origin main\n",
    "# Change the working directory to the parent directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "# %%capture\n",
    "%pip install pytorch-lightning\n",
    "%pip install pycocotools\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the standard libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Import the PyTorch libraries and modules\n",
    "import torch\n",
    "\n",
    "# Import the PyTorch Lightning libraries and modules\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import the coco library\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Import the W&B (Weights & Biases) library\n",
    "# import wandb\n",
    "# from wandb.sdk import wandb_run\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import cv2\n",
    "import base64\n",
    "\n",
    "# Import the tqdm library (for the progress bars)\n",
    "if not RUNNING_IN_COLAB:\n",
    "\tfrom tqdm import tqdm\n",
    "else:\n",
    "\tfrom tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom modules\n",
    "if not RUNNING_IN_COLAB:\n",
    "\t# We are running locally (not on Google Colab, import modules from the \"src\" directory in the current directory)\n",
    "\tfrom src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore\n",
    "else:\n",
    "\t# We are running on Google Colab (import modules from the pulled repository stored in the project's directory)\n",
    "\tfrom computer_vision_project_dev.src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom computer_vision_project_dev.src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "\n",
    "# ===== Training & Datasets constants =====================================================\n",
    "\n",
    "# MS COCO dataset constants (use MS COCO 2014 dataset for image captioning)\n",
    "COCO_DATA_YEAR = '2014'  \t# '2014' or '2017'\n",
    "COCO_DATA_TYPE = 'val'  # 'train' or 'val'\n",
    "COCO_DATA_CAPTIONS_FILE = f\"/annotations/captions_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the annotations file inside the DATA_FOLDER\n",
    "CODO_DATA_INSTANCES_FILE = f\"/annotations/instances_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the instances file inside the DATA_FOLDER\n",
    "\n",
    "# Size of the image patches\n",
    "IMAGE_PATCH_SIZE = 16\n",
    "# Number of image patches per dimension (i.e. both vertically and horizontally, since images have a square aspect ratio)\n",
    "IMAGE_PATCHES_PER_DIMENSION = 10\t# 3x3 patches, 48x48 pixels images\n",
    "\n",
    "# Total number of images to consider in the dataset (will be split into training, validation and test sets), set to -1 to use all the available images\n",
    "NUMBER_OF_IMAGES_IN_DB = 100\t\t# Was 1000\n",
    "# Minimum number of captions for an image\n",
    "MIN_IMAGE_CAPTIONS = 5\n",
    "# If not enough square images are found, also accept images that have this max aspect difference (they will be cropped to a square aspect ratio later)\n",
    "MAX_ASPECT_RATIO_TOLERANCE = 0.1 \t# Accept images that are 10% wider than they are tall (or vice versa)\n",
    "# Minimum number of images per class\n",
    "MIN_IMAGES_PER_CLASS = 10\t# Was 100\n",
    "\n",
    "# Percentage of images, for each class, to use for the image retrieval dataset (the remaining images will be used for the indexing dataset, i.e. will be added in the images database)\n",
    "IMAGE_RETRIEVAL_DB_PERCENTAGE = 0.8\n",
    "\n",
    "# ===== Evaluation constants ==============================================================\n",
    "\n",
    "# Define the number of images K to retrieve for each query and the number of queries N to calculate the mean average precision (MAP@K)\n",
    "MAP_K = 10\n",
    "MAP_N = 10\n",
    "\n",
    "# Define the number of images K to retrieve for each query to calculate the Recall@K metrics\n",
    "RECALL_K = 1_000\n",
    "\n",
    "# Whether to print the debug information during the MAP@K and Recall@K evaluation of the models\n",
    "PRINT_EVALUATION_DEBUG = True\n",
    "\n",
    "# Whether to evaluate the models (i.e. compute the MAP@K and Recall@K metrics for the trained models on the test datasets)\n",
    "EVALUATE_MODELS = True\n",
    "\n",
    "# ===== MAIN CONSTANTS =====================================================================\n",
    "\n",
    "# Define the data folder, onto which the various dictionaries, lists and other data will be saved\n",
    "DATA_FOLDER = \"src/data\" if not RUNNING_IN_COLAB else \"/content/data\"\n",
    "\n",
    "# Define the path to save models\n",
    "MODELS_FOLDER = \"src/models\" if not RUNNING_IN_COLAB else \"/content/models\"\n",
    "\n",
    "# Force the creation of the \"image_db\" images list, the JSON files for the datasets, ecc...\n",
    "FORCE_DICTIONARIES_CREATION = True\t\t# Set to false to try to load the dictionaries from the DATA_FOLDER if they exist\n",
    "\n",
    "# Whether to load model checkpoints (if they were already saved locally) or not\n",
    "LOAD_MODELS_CHECKPOINTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WANDB_API_KEY (set to \"\" to disable W&B logging)\n",
    "# NOTE: leaving the WANDB_API_KEY to a value of None will throw an error\n",
    "WANDB_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wandb logger, api object, entity name and project name\n",
    "wandb_logger = None\n",
    "wandb_api = None\n",
    "wandb_entity = None\n",
    "wandb_project = None\n",
    "# Check if a W&B api key is provided\n",
    "if WANDB_API_KEY == None:\n",
    "\tprint(\"No W&B API key provided, please provide a valid key to use the W&B API or set the WANDB_API_KEY variable to an empty string to disable logging\")\n",
    "\traise ValueError(\"No W&B API key provided...\")\n",
    "elif WANDB_API_KEY != \"\":\n",
    "\t# Login to the W&B (Weights & Biases) API\n",
    "\twandb.login(key=WANDB_API_KEY, relogin=True)\n",
    "\t# Minimize the logging from the W&B (Weights & Biases) library\n",
    "\tos.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\tlogging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "\t# Initialize the W&B (Weights & Biases) loggger\n",
    "\twandb_logger = WandbLogger(\n",
    "\t\tlog_model=\"all\", project=\"cv-dsi-project\", name=\"- SEPARATOR -\")\n",
    "\t# Initialize the W&B (Weights & Biases) API\n",
    "\twandb_api = wandb.Api()\n",
    "\t# Get the W&B (Weights & Biases) entity name\n",
    "\twandb_entity = wandb_logger.experiment.entity\n",
    "\t# Get the W&B (Weights & Biases) project name\n",
    "\twandb_project = wandb_logger.experiment.project\n",
    "\t# Finish the \"separator\" experiment\n",
    "\twandb_logger.experiment.finish(quiet=True)\n",
    "\tprint(\"W&B API key provided, logging with W&B enabled.\")\n",
    "else:\n",
    "\tprint(\"No W&B API key provided, logging with W&B disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they do not exist\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "\tprint(f\"Creating the data folder at '{DATA_FOLDER}'...\")\n",
    "\tos.makedirs(DATA_FOLDER)\n",
    "if not os.path.exists(MODELS_FOLDER):\n",
    "\tprint(f\"Creating the models folder at '{MODELS_FOLDER}'...\")\n",
    "\tos.makedirs(MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the annotation file for the COCO dataset exists, if it does not exist, download it\n",
    "!cd {DATA_FOLDER} && wget -nc http://images.cocodataset.org/annotations/annotations_trainval{COCO_DATA_YEAR}.zip\n",
    "!cd {DATA_FOLDER} && unzip -n annotations_trainval{COCO_DATA_YEAR}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the COCO api for captioning\n",
    "coco_captions = COCO(f\"{DATA_FOLDER}{COCO_DATA_CAPTIONS_FILE}\")\n",
    "# Initialize the COCO api for object detection\n",
    "coco_instances = COCO(f\"{DATA_FOLDER}{CODO_DATA_INSTANCES_FILE}\")\n",
    "\n",
    "# Show the COCO dataset info for the captioning task\n",
    "print(\"\\nCOCO captioning dataset infos:\")\n",
    "coco_captions.info()\n",
    "\n",
    "# Show the information for the captioning task\n",
    "print(\"\\nCOCO captioning task infos:\")\n",
    "coco_caps = coco_captions.dataset['annotations']\n",
    "print(\"Number of images: \", len(coco_captions.getImgIds()))\n",
    "print(\"Number of captions: \", len(coco_caps))\n",
    "print(\"Number of average captions per image: \", len(coco_caps) / len(coco_captions.getImgIds()))\n",
    "\n",
    "# Show the COCO dataset info for the object detection task\n",
    "print(\"\\nCOCO object detection dataset infos:\")\n",
    "coco_instances.info()\n",
    "\n",
    "# Show the information for the object detection task\n",
    "print(\"\\nCOCO object detection task infos:\")\n",
    "coco_objs = coco_instances.dataset['annotations']\n",
    "print(\"Number of images: \", len(coco_instances.getImgIds()))\n",
    "print(\"Number of objects: \", len(coco_objs))\n",
    "print(\"Number of categories: \", len(coco_instances.cats))\n",
    "print(\"Categories:\")\n",
    "utils.print_json(coco_instances.cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some examples from the MS COCO dataset\n",
    "\n",
    "# Print the first image object example\n",
    "example_image_index = 0\n",
    "print(\"\\nImage object example: \")\n",
    "image_example = coco_captions.loadImgs(coco_caps[example_image_index]['image_id'])[0]\n",
    "utils.print_json(image_example, 2)\n",
    "\n",
    "# Print the actual image file\n",
    "print(\"\\nActual image of the example (size: \" + str(image_example['width']) + \"x\" + str(image_example['height']) + \"):\")\n",
    "url = image_example['coco_url']\n",
    "image = io.imread(url)\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Downscale the image to the maximum allowed size in the model\n",
    "image_max_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "# Crop the image to a square aspect ratio if it is not already square\n",
    "downscaled_image = image\n",
    "if image_example['width'] > image_example['height']:\n",
    "\t# Image is wider than tall, crop the sides\n",
    "\tcrop_width = (image_example['width'] - image_example['height']) // 2\n",
    "\tdownscaled_image = image[:, crop_width:crop_width+image_example['height']]\n",
    "elif image_example['height'] > image_example['width']:\n",
    "\t# Image is taller than wide, crop the top and bottom\n",
    "\tcrop_height = (image_example['height'] - image_example['width']) // 2\n",
    "\tdownscaled_image = image[crop_height:crop_height+image_example['width'], :]\n",
    "# Downscale the image to the maximum allowed size\n",
    "downscaled_image = cv2.resize(downscaled_image, (image_max_size, image_max_size))\n",
    "print(\"\\nDownscaled & cropped image of the example (size: \" + str(image_max_size) + \"x\" + str(image_max_size) + \"):\")\n",
    "plt.axis('off')\n",
    "plt.imshow(downscaled_image)\n",
    "plt.show()\n",
    "\n",
    "# Print the captions for the given image\n",
    "print(\"\\nCaption examples for the given image: \")\n",
    "captions_for_image = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=image_example['id']))\n",
    "for caption, i in zip(captions_for_image, range(len(captions_for_image))):\n",
    "\tprint(str(i+1) + \") \" + caption['caption'].strip())\n",
    "\n",
    "# Print the captioning object example\n",
    "print(\"\\nFirst caption object example:\")\n",
    "utils.print_json(captions_for_image[0], 2)\n",
    "\n",
    "# Print information about the object detection task for the given image\n",
    "print(\"\\nObject detection examples for the given image:\")\n",
    "# Get the object detection annotations for the given image\n",
    "annotations_for_image = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=image_example['id']))\n",
    "print(\"List of the \" + str(len(annotations_for_image)) + \" object detection annotations for the given image (obtained using the 'coco_instances.loadAnns(image_annotation_id)' function):\")\n",
    "for annotation, i in zip(annotations_for_image, range(len(annotations_for_image))):\n",
    "\tprint(\"\\n> Annotation \" + str(i+1) + \":\")\n",
    "\t# Print the annotation object\n",
    "\tutils.print_json(annotation, 2, truncate_large_lists=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset of images for the training of the Vision Transformer model\n",
    "\n",
    "# Function that returns the list containing the images for the training of the Vision Transformer model\n",
    "def get_images_db(number_of_images, process_images=True):\n",
    "\t'''\n",
    "\t\tBuilds a list of images for the training of the Vision Transformer model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tnumber_of_images (int): The number of images to include in the dataset (search is stopped when the number of images is reached), use -1 to include all available images\n",
    "\t\t\tprocess_images (bool): Whether to process the images (i.e. retrieve actual image data, crop images and compupte their base64 encodings to add to the list)\n",
    "\t'''\n",
    "\t# Structure of the images\n",
    "\timages_list_object = {\n",
    "\t\t\"image_id\": \"\",\t\t\t# ID of the image (as found in the COCO dataset)\n",
    "\t\t\"image_url\": \"\",\t\t# URL of the image\n",
    "\t\t\"image_width\": 0,\t\t# The original image width\n",
    "\t\t\"image_height\": 0,\t\t# The original image height\n",
    "\t\t\"image_captions\": [],\t# List of captions for the image\n",
    "\t\t\"image_classes\": [\t\t# List of classes for the image (i.e. detected objects, in the order of area size)\n",
    "\t\t\t{\n",
    "\t\t\t\t\"class_id\": 0,\t\t# ID of the class (as found in the COCO dataset)\n",
    "\t\t\t\t\"class_name\": \"\",\t# Name of the class\n",
    "\t\t\t\t\"class_area\": 0,\t# Sum of the area of each instance of the class in the image\n",
    "\t\t\t\t\"class_count\": 0\t# Number of instances of the class in the image\n",
    "\t\t\t}\n",
    "\t\t],\t\n",
    "\t\t\"image_data\": \"\"\t\t# Base64 string of the image\n",
    "\t}\n",
    "\t# Get the image ids\n",
    "\timg_ids = coco_captions.getImgIds()\n",
    "\t# Get the images\n",
    "\timages = []\n",
    "\t\n",
    "\t# Function that returns a list of images with the given aspect ratio tolerance\n",
    "\tdef select_images_list(image_aspect_ratio_tolerance):\n",
    "\t\t# Get the images\n",
    "\t\tfor img_id in img_ids:\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg_obj = coco_captions.loadImgs(img_id)[0]\n",
    "\t\t\t# Check if the size of the image is square or within the aspect ratio tolerance\n",
    "\t\t\timage_aspect_ratio = img_obj['width'] / img_obj['height']\n",
    "\t\t\tif abs(image_aspect_ratio - 1) > image_aspect_ratio_tolerance:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Check if the image is already in the images list\n",
    "\t\t\tif any(img['image_id'] == img_obj['id'] for img in images):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Get the image url\n",
    "\t\t\timg_url = img_obj['coco_url']\n",
    "\t\t\t# Get the captions for the image\n",
    "\t\t\timg_captions = []\n",
    "\t\t\tcaptions = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tfor caption in captions:\n",
    "\t\t\t\tcaption_text = caption['caption'].strip()\n",
    "\t\t\t\tif len(caption_text) > 1:\n",
    "\t\t\t\t\timg_captions.append(caption_text)\n",
    "\t\t\t# Discard the image if the number of captions is less than the minimum\n",
    "\t\t\tif len(img_captions) < MIN_IMAGE_CAPTIONS:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Discard the image if it has no classes\n",
    "\t\t\tclasses = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tif len(classes) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Create a classes object with the fields: \"class_id\", \"class_name\", \"class_area\"\n",
    "\t\t\tclasses_obj = {}\n",
    "\t\t\tfor class_obj in classes:\n",
    "\t\t\t\tclass_id = class_obj['category_id']\n",
    "\t\t\t\tclass_name = coco_instances.cats[class_id]['name']\n",
    "\t\t\t\tclass_area = class_obj['area']\n",
    "\t\t\t\tif class_id not in classes_obj:\n",
    "\t\t\t\t\tclasses_obj[class_id] = {\n",
    "\t\t\t\t\t\t\"class_id\": class_id,\n",
    "\t\t\t\t\t\t\"class_name\": class_name,\n",
    "\t\t\t\t\t\t\"class_area\": class_area,\n",
    "\t\t\t\t\t\t\"class_count\": 1,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_area'] += class_area\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_count'] += 1\n",
    "\t\t\t# Convert the classes object to a list\n",
    "\t\t\tclasses_obj = list(classes_obj.values())\n",
    "\t\t\t# Sort the classes by area size\n",
    "\t\t\tclasses_obj = sorted(classes_obj, key=lambda x: x['class_area'], reverse=True)\n",
    "\t\t\t# Add the image to the images list\n",
    "\t\t\timages_list_object = {\n",
    "\t\t\t\t\"image_id\": img_obj['id'],\n",
    "\t\t\t\t\"image_url\": img_url,\n",
    "\t\t\t\t\"image_width\": img_obj['width'],\n",
    "\t\t\t\t\"image_height\": img_obj['height'],\n",
    "\t\t\t\t\"image_captions\": img_captions,\n",
    "\t\t\t\t\"image_classes\": classes_obj,\n",
    "\t\t\t\t\"image_data\": None # Will be filled later\n",
    "\t\t\t}\n",
    "\t\t\timages.append(images_list_object)\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif number_of_images >= 1 and len(images) >= number_of_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Return the images list\n",
    "\t\treturn images\n",
    "\tprint(\"Selecting images with a square aspect ratio...\")\n",
    "\t# Get the images that have a square aspect ratio first\n",
    "\timages = select_images_list(0)\n",
    "\t# Get the remaining images with the given aspect ratio tolerance\n",
    "\tif len(images) < number_of_images or number_of_images == -1:\n",
    "\t\tsquare_aspect_ratio_images = len(images)\n",
    "\t\tprint(\"> Found \" + str(square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio, looking for the remaining images...\")\n",
    "\t\tprint(\"Looking for remaining images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"% (either a \" + str(1 + MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio or a \" + str(1 - MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio)...\")\n",
    "\t\timages = select_images_list(MAX_ASPECT_RATIO_TOLERANCE)\n",
    "\t\tnon_square_aspect_ratio_images = len(images) - square_aspect_ratio_images\n",
    "\t\t# Print the number of images found\n",
    "\t\tprint(\"> Found \" + str(non_square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" )  + \" more images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"%.\")\n",
    "\telse:\n",
    "\t\tprint(\"> Found \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio.\")\n",
    "\t# Print a message based on the number of images found\n",
    "\tif len(images) < number_of_images and number_of_images != -1:\n",
    "\t\tprint(\"WARNING: Could not find enough images with the required aspect ratio tolerance, only \" + str(len(images)) + \" / \" + str(number_of_images) + \" images found.\")\n",
    "\telse:\n",
    "\t\tprint(\"DONE: Found all \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with the required aspect ratio tolerance.\")\n",
    "\t# Get all the image data\n",
    "\tif process_images:\n",
    "\t\tfor img in tqdm(images, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\t\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\t# Return the images list\n",
    "\treturn images\n",
    "\n",
    "# List of image objects used for the training of the Vision Transformer model\n",
    "images_db = []\n",
    "\n",
    "# Check if the images list should be rebuilt or loaded\n",
    "images_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "if os.path.exists(images_db_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\twith open(images_db_file, 'r') as f:\n",
    "\t\timages_db = json.load(f)\n",
    "\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "else:\n",
    "\t# Initialize the images list\n",
    "\timages_db = get_images_db(-1, False)\n",
    "\t# Save the images list to a JSON file\n",
    "\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\tprint(\"Saving the images list to the file: \", images_db_file)\n",
    "\twith open(images_db_file, 'w') as f:\n",
    "\t\tjson.dump(images_db, f)\n",
    "\n",
    "# Print the final number of images in the dataset\n",
    "print(\"\\nNumber of loaded images in the dataset: \" + str(len(images_db)) + (\"/\" + str(NUMBER_OF_IMAGES_IN_DB) if NUMBER_OF_IMAGES_IN_DB != -1 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"classes\" list with the classes found in the dataset and, for each of them, a list of the images in which they appear\n",
    "\n",
    "# Function to get the classes list from the images\n",
    "def get_classes_dict():\n",
    "\t# Initialize the classes list\n",
    "\tclasses = {}\n",
    "\t# Get the classes from the images\n",
    "\tfor i in tqdm(range(len(images_db)), desc=\"Processing images for classes...\"):\n",
    "\t\timg = images_db[i]\n",
    "\t\tfor class_obj in img['image_classes']:\n",
    "\t\t\t# Get the class id\n",
    "\t\t\tclass_id = class_obj['class_id']\n",
    "\t\t\t# Add the class to the classes list if it does not exist\n",
    "\t\t\tif class_id not in classes.keys():\n",
    "\t\t\t\tclasses[class_id] = []\n",
    "\t\t\t# Add the image index to the class list\n",
    "\t\t\tclasses[class_id].append(i)\n",
    "\tprint(\"Created the classes list from the images with \" + str(len(classes)) + \" classes.\")\n",
    "\t# Sort classes by the number of images\n",
    "\tclasses = {k: v for k, v in sorted(classes.items(), key=lambda item: len(item[1]), reverse=True)}\n",
    "\t# Discard the classes with less than the minimum number of images\n",
    "\tclasses = {k: v for k, v in classes.items() if len(v) >= MIN_IMAGES_PER_CLASS}\n",
    "\tprint(\"Discarded the classes with less than \" + str(MIN_IMAGES_PER_CLASS) + \" images: \" + str(len(classes)) + \" / \" + str(len(classes.keys()) + len(classes)) + \" classes remaining.\")\n",
    "\t# Return the classes list\n",
    "\treturn classes\n",
    "\n",
    "# Get the classes list if it already exists, otherwise create it\n",
    "classes = {}\n",
    "classes_file = os.path.join(DATA_FOLDER, \"classes.json\")\n",
    "if os.path.exists(classes_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\twith open(classes_file, 'r') as f:\n",
    "\t\tclasses = json.load(f)\n",
    "\tif len(classes) > 0:\n",
    "\t\tprint(\"Loaded the classes list from the file: \", classes_file)\n",
    "else:\n",
    "\tprint(\"Creating the classes dictionary from the images...\")\n",
    "\tclasses = get_classes_dict()\n",
    "\t# Save the classes list to a JSON file\n",
    "\tprint(\"Saving the classes list to the file: \", classes_file)\n",
    "\twith open(classes_file, 'w') as f:\n",
    "\t\tjson.dump(classes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the images in the images DB to finally only include the images that have the classes in the classes list, with MIN_IMAGES_PER_CLASS images per class, and to populate the images list with the base64 encoding of the images\n",
    "\n",
    "# Function to update the images list to only include the images that have the classes in the classes list\n",
    "def update_images_db_based_on_classes(max_images):\n",
    "\t# Number of classes to maintain the designated number of images\n",
    "\tclasses_count = math.ceil(max_images / MIN_IMAGES_PER_CLASS)\n",
    "\t# Initialize the new images list\n",
    "\tnew_images_db = []\n",
    "\t# Get the classes to maintain the designated number of images\n",
    "\tclasses_to_maintain = list(classes.keys())[:classes_count]\n",
    "\t# Create a new classes list with the classes found in the new images list\n",
    "\tnew_classes = {}\n",
    "\t# Get the images to maintain the designated number of images\n",
    "\tfor i in tqdm(range(len(images_db)), desc=\"Processing images for classes...\"):\n",
    "\t\timg = images_db[i]\n",
    "\t\t# Check if the image has any of the classes to maintain\n",
    "\t\tif any(class_obj['class_id'] in classes_to_maintain for class_obj in img['image_classes']):\n",
    "\t\t\tnew_images_db.append(img)\n",
    "\t\t# Break if the number of images is reached\n",
    "\t\tif len(new_images_db) >= max_images:\n",
    "\t\t\tbreak\n",
    "\t# Get the classes from the classes to maintain\n",
    "\tfor class_id in classes_to_maintain:\n",
    "\t\t# Remove any image index that is not in the new images list\n",
    "\t\tnew_classes[class_id] = [i for i in classes[class_id] if i < len(new_images_db)]\n",
    "\t# Return the new images list\n",
    "\treturn new_images_db, new_classes\n",
    "\n",
    "# Update the images list to only include the images that have the classes in the classes list\n",
    "max_images = NUMBER_OF_IMAGES_IN_DB if NUMBER_OF_IMAGES_IN_DB != -1 else len(images_db)\n",
    "print(\"\\nUpdating the images list to only include the \" + str(len(classes)) + \" classes with at least \" + str(MIN_IMAGES_PER_CLASS) + \" images, not exceeding \" + str(max_images) + \" images...\")\n",
    "images_db, classes = update_images_db_based_on_classes(max_images)\n",
    "print(\"DONE: Updated the images list, now containing \" + str(len(images_db)) + \" images.\")\n",
    "print(\"> Final number of classes in the dataset: \" + str(len(classes)))\n",
    "\n",
    "# Update the images list to include the base64 encoding of the images\n",
    "print(\"Computing the BASE64 images encoding for the images list...\")\n",
    "for img in tqdm(images_db, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "print(\"DONE: Computed the BASE64 images encoding for the images list.\")\n",
    "\n",
    "# Save the updated images list to a JSON file\n",
    "images_db_file = os.path.join(DATA_FOLDER, \"images_db_updated.json\")\n",
    "print(\"Saving the updated images list to the file: \", images_db_file)\n",
    "with open(images_db_file, 'w') as f:\n",
    "\tjson.dump(images_db, f)\n",
    "\n",
    "# Print the final number of images in the dataset\n",
    "print(\"\\nNumber of loaded images in the dataset: \" + str(len(images_db)))\n",
    "print(\"Number of classes in the dataset: \" + str(len(classes)))\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first image object example\n",
    "example_image_index = -1\n",
    "print(\"Image object example: \")\n",
    "utils.print_json(images_db[example_image_index], 2)\n",
    "\n",
    "# Print the actual image file\n",
    "image_b64_string = images_db[example_image_index]['image_data'] if images_db[example_image_index]['image_data'] != None else utils.get_image_data_as_base64(images_db[example_image_index]['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "image = utils.get_image_from_b64_string(image_b64_string)\n",
    "print(\"\\nActual image of the example (original size: \" + str(images_db[example_image_index]['image_width']) + \"x\" + str(images_db[example_image_index]['image_height']) + \" | downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Print how the Transformer model sees the image\n",
    "print(\"\\nHow the Transformer model sees the image (downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "# Divide the image into smaller images representing the patches\n",
    "image_patches = []\n",
    "for i in range(0, image.shape[0], IMAGE_PATCH_SIZE):\n",
    "\tfor j in range(0, image.shape[1], IMAGE_PATCH_SIZE):\n",
    "\t\timage_patch = image[i:i+IMAGE_PATCH_SIZE, j:j+IMAGE_PATCH_SIZE]\n",
    "\t\timage_patches.append(image_patch)\n",
    "# Display the image patches\n",
    "fig, axs = plt.subplots(IMAGE_PATCHES_PER_DIMENSION, IMAGE_PATCHES_PER_DIMENSION, figsize=(10, 10))\n",
    "for i in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\tfor j in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\t\taxs[i, j].imshow(image_patches[i*IMAGE_PATCHES_PER_DIMENSION+j])\n",
    "\t\taxs[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classes list\n",
    "print(\"\\nClasses list sorted by number of images (\" + str(len(classes)) + \" classes out of \" + str(len(coco_instances.cats)) + \" total MS COCO classes):\")\n",
    "utils.print_json(classes, 2, truncate_large_lists=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the images list into a list for the indexing dataset (i.e. images in the database) and a list for the image retrieval dataset (i.e. similar images to retrieve images in the DB)\n",
    "\n",
    "# List of images for the indexing and image retrieval datasets\n",
    "images_db_indexing = []\t# List of images for the indexing dataset\n",
    "images_db_image_retrieval = {} # Dictionary containing image IDs of images NOT in the indexing dataset as keys and the list of similar images in the indexing dataset as values\n",
    "for class_id in classes.keys():\n",
    "\tclass_obj = classes[class_id]\n",
    "\tindexing_number = int(len(class_obj) * (1 - IMAGE_RETRIEVAL_DB_PERCENTAGE))\n",
    "\tsimilar_images = []\n",
    "\tfor i in range(len(class_obj)):\n",
    "\t\tis_in_db = i < indexing_number\n",
    "\t\timg_id = class_obj[i]\n",
    "\t\t# Get the image object\n",
    "\t\timg = images_db[img_id]\n",
    "\t\tif is_in_db:\n",
    "\t\t\t# Add the image to the indexing dataset\n",
    "\t\t\timages_db_indexing.append(img)\n",
    "\t\t\t# Add the image to the similar images list\n",
    "\t\t\tsimilar_images.append(img)\n",
    "\t\telse:\n",
    "\t\t\t# Add the image to the image retrieval dataset\n",
    "\t\t\timages_db_image_retrieval[img_id] = similar_images\n",
    "\n",
    "# Compute the max length of the image IDS (we consider the index of the image in the \"images_db\" as the image ID)\n",
    "max_image_id_length = len(images_db)\n",
    "# Number of output tokens for the encoded image IDs (the 10 digits [0-9] plus the 3 special tokens, i.e. end of sequence, padding, start of sequence)\n",
    "output_tokens = 10 + 3\n",
    "\n",
    "# Build the Transformer Indexing Database for training the vision transformer\n",
    "transformer_indexing_dataset = datasets.TransformerIndexingDataset(\n",
    "\timages=images_db_indexing,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=IMAGE_PATCHES_PER_DIMENSION,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=os.path.join(DATA_FOLDER, \"transformer_indexing_dataset.json\"),\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION\n",
    ")\n",
    "\n",
    "# Build the Transformer Image Retrieval Database for training the vision transformer\n",
    "transformer_image_retrieval_dataset = datasets.TransformerImageRetrievalDataset(\n",
    "\tall_images=images_db,\n",
    "\tsimilar_images=images_db_image_retrieval,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=IMAGE_PATCHES_PER_DIMENSION,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=os.path.join(DATA_FOLDER, \"transformer_image_retrieval_dataset.json\"),\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first example from the Transformer Indexing Dataset\n",
    "example_index = 0\n",
    "print(\"Example from the Transformer Indexing Dataset:\")\n",
    "print(\"<encoded_image, encoded_image_id> tuple:\")\n",
    "print(transformer_indexing_dataset[example_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
