{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on colab or locally\n",
    "try:\n",
    "\tfrom google.colab import files\n",
    "\tRUNNING_IN_COLAB = True\n",
    "\tprint(\"Running on Google Colab.\")\n",
    "except ModuleNotFoundError:\n",
    "\tRUNNING_IN_COLAB = False\n",
    "\tprint(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the git repository of the project for the source files\n",
    "!git clone https://github.com/valeriodiste/computer_vision_project_dev.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to the cloned repository\n",
    "# TO DO: change the directory to the correct one\n",
    "%cd /content/computer_vision_project_dev\n",
    "# Pull the latest changes from the repository\n",
    "!git pull origin main\n",
    "# Change the working directory to the parent directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "# %%capture\n",
    "%pip install pytorch-lightning\n",
    "%pip install pycocotools\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the standard libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Import the PyTorch libraries and modules\n",
    "import torch\n",
    "\n",
    "# Import the PyTorch Lightning libraries and modules\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import the coco library\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Import the W&B (Weights & Biases) library\n",
    "# import wandb\n",
    "# from wandb.sdk import wandb_run\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import cv2\n",
    "import base64\n",
    "\n",
    "# Import the tqdm library (for the progress bars)\n",
    "if not RUNNING_IN_COLAB:\n",
    "\tfrom tqdm import tqdm\n",
    "else:\n",
    "\tfrom tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom modules\n",
    "if not RUNNING_IN_COLAB:\n",
    "\t# We are running locally (not on Google Colab, import modules from the \"src\" directory in the current directory)\n",
    "\tfrom src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore\n",
    "else:\n",
    "\t# We are running on Google Colab (import modules from the pulled repository stored in the project's directory)\n",
    "\tfrom computer_vision_project_dev.src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom computer_vision_project_dev.src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "\n",
    "# ===== Training & Datasets constants =====================================================\n",
    "\n",
    "# MS COCO dataset constants (use MS COCO 2014 dataset for image captioning)\n",
    "COCO_DATA_YEAR = '2014'  \t# '2014' or '2017'\n",
    "COCO_DATA_TYPE = 'val'  # 'train' or 'val'\n",
    "COCO_DATA_CAPTIONS_FILE = f\"/annotations/captions_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the annotations file inside the DATA_FOLDER\n",
    "CODO_DATA_INSTANCES_FILE = f\"/annotations/instances_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the instances file inside the DATA_FOLDER\n",
    "\n",
    "# Size of the image patches\n",
    "IMAGE_PATCH_SIZE = 16\n",
    "# Number of image patches per dimension (i.e. both vertically and horizontally, since images have a square aspect ratio)\n",
    "IMAGE_PATCHES_PER_DIMENSION = 10\t# 3x3 patches, 48x48 pixels images\n",
    "\n",
    "# Total number of images to consider in the dataset (will be split into training, validation and test sets), set to -1 to use all the available images\n",
    "NUMBER_OF_IMAGES_IN_DB = 100\t\t# Was 1000\n",
    "# Minimum number of captions for an image\n",
    "MIN_IMAGE_CAPTIONS = 5\n",
    "# If not enough square images are found, also accept images that have this max aspect difference (they will be cropped to a square aspect ratio later)\n",
    "MAX_ASPECT_RATIO_TOLERANCE = 0.1 \t# Accept images that are 10% wider than they are tall (or vice versa)\n",
    "# Minimum number of images per class\n",
    "MIN_IMAGES_PER_CLASS = 10\t# Was 100\n",
    "\n",
    "# Percentage of images, for each class, to use for the image retrieval dataset (the remaining images will be used for the indexing dataset, i.e. will be added in the images database)\n",
    "IMAGE_RETRIEVAL_DB_PERCENTAGE = 0.8\n",
    "\n",
    "# Whether to load demo data from the \"demo/\" folder (set to False to build the dataset from the COCO dataset using the above constants or to load its existing version)\n",
    "USE_DEMO_DATA = True\n",
    "# Number of images in the demo dataset (if LOAD_DEMO_DATA is set to True)\n",
    "DEMO_DATA_SIZE = 100\t# Data will be found in the \"demo/{DEMO_DATA_SIZE}/\" folder\n",
    "\n",
    "# ===== Evaluation constants ==============================================================\n",
    "\n",
    "# Define the number of images K to retrieve for each query and the number of queries N to calculate the mean average precision (MAP@K)\n",
    "MAP_K = 10\n",
    "MAP_N = 10\n",
    "\n",
    "# Define the number of images K to retrieve for each query to calculate the Recall@K metrics\n",
    "RECALL_K = 1_000\n",
    "\n",
    "# Whether to print the debug information during the MAP@K and Recall@K evaluation of the models\n",
    "PRINT_EVALUATION_DEBUG = True\n",
    "\n",
    "# Whether to evaluate the models (i.e. compute the MAP@K and Recall@K metrics for the trained models on the test datasets)\n",
    "EVALUATE_MODELS = True\n",
    "\n",
    "# ===== MAIN CONSTANTS =====================================================================\n",
    "\n",
    "# Define the data folder, onto which the various dictionaries, lists and other data will be saved\n",
    "DATA_FOLDER = \"src/data\" if not RUNNING_IN_COLAB else \"/content/data\"\n",
    "\n",
    "# Define the path to save models\n",
    "MODELS_FOLDER = \"src/models\" if not RUNNING_IN_COLAB else \"/content/models\"\n",
    "\n",
    "# Folder containing the demo images and captions\n",
    "DEMO_FOLDER = f\"src/demo/{DEMO_DATA_SIZE}/\" if not RUNNING_IN_COLAB else f\"/content/computer_vision_project_dev/src/demo/{DEMO_DATA_SIZE}/\"\n",
    "\n",
    "# Force the creation of the \"image_db\" images list, the JSON files for the datasets, ecc...\n",
    "FORCE_DICTIONARIES_CREATION = True\t\t# Set to false to try to load the dictionaries from the DATA_FOLDER if they exist\n",
    "\n",
    "# Whether to load model checkpoints (if they were already saved locally) or not\n",
    "LOAD_MODELS_CHECKPOINTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WANDB_API_KEY (set to \"\" to disable W&B logging)\n",
    "# NOTE: leaving the WANDB_API_KEY to a value of None will throw an error\n",
    "WANDB_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wandb logger, api object, entity name and project name\n",
    "wandb_logger = None\n",
    "wandb_api = None\n",
    "wandb_entity = None\n",
    "wandb_project = None\n",
    "# Check if a W&B api key is provided\n",
    "if WANDB_API_KEY == None:\n",
    "\tprint(\"No W&B API key provided, please provide a valid key to use the W&B API or set the WANDB_API_KEY variable to an empty string to disable logging\")\n",
    "\traise ValueError(\"No W&B API key provided...\")\n",
    "elif WANDB_API_KEY != \"\":\n",
    "\t# Login to the W&B (Weights & Biases) API\n",
    "\twandb.login(key=WANDB_API_KEY, relogin=True)\n",
    "\t# Minimize the logging from the W&B (Weights & Biases) library\n",
    "\tos.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\tlogging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "\t# Initialize the W&B (Weights & Biases) loggger\n",
    "\twandb_logger = WandbLogger(\n",
    "\t\tlog_model=\"all\", project=\"cv-dsi-project\", name=\"- SEPARATOR -\")\n",
    "\t# Initialize the W&B (Weights & Biases) API\n",
    "\twandb_api = wandb.Api()\n",
    "\t# Get the W&B (Weights & Biases) entity name\n",
    "\twandb_entity = wandb_logger.experiment.entity\n",
    "\t# Get the W&B (Weights & Biases) project name\n",
    "\twandb_project = wandb_logger.experiment.project\n",
    "\t# Finish the \"separator\" experiment\n",
    "\twandb_logger.experiment.finish(quiet=True)\n",
    "\tprint(\"W&B API key provided, logging with W&B enabled.\")\n",
    "else:\n",
    "\tprint(\"No W&B API key provided, logging with W&B disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they do not exist\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "\tprint(f\"Creating the data folder at '{DATA_FOLDER}'...\")\n",
    "\tos.makedirs(DATA_FOLDER)\n",
    "if not os.path.exists(MODELS_FOLDER):\n",
    "\tprint(f\"Creating the models folder at '{MODELS_FOLDER}'...\")\n",
    "\tos.makedirs(MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the annotation file for the COCO dataset exists, if it does not exist, download it\n",
    "!cd {DATA_FOLDER} && wget -nc http://images.cocodataset.org/annotations/annotations_trainval{COCO_DATA_YEAR}.zip\n",
    "!cd {DATA_FOLDER} && unzip -n annotations_trainval{COCO_DATA_YEAR}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset or load the demo dataset\n",
    "coco_captions = None\n",
    "coco_instances = None\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\n",
    "\t# Initialize the COCO api for captioning\n",
    "\tcoco_captions = COCO(f\"{DATA_FOLDER}{COCO_DATA_CAPTIONS_FILE}\")\n",
    "\t# Initialize the COCO api for object detection\n",
    "\tcoco_instances = COCO(f\"{DATA_FOLDER}{CODO_DATA_INSTANCES_FILE}\")\n",
    "\n",
    "\t# Show the COCO dataset info for the captioning task\n",
    "\tprint(\"\\nCOCO captioning dataset infos:\")\n",
    "\tcoco_captions.info()\n",
    "\n",
    "\t# Show the information for the captioning task\n",
    "\tprint(\"\\nCOCO captioning task infos:\")\n",
    "\tcoco_caps = coco_captions.dataset['annotations']\n",
    "\tprint(\"Number of images: \", len(coco_captions.getImgIds()))\n",
    "\tprint(\"Number of captions: \", len(coco_caps))\n",
    "\tprint(\"Number of average captions per image: \", len(coco_caps) / len(coco_captions.getImgIds()))\n",
    "\n",
    "\t# Show the COCO dataset info for the object detection task\n",
    "\tprint(\"\\nCOCO object detection dataset infos:\")\n",
    "\tcoco_instances.info()\n",
    "\n",
    "\t# Show the information for the object detection task\n",
    "\tprint(\"\\nCOCO object detection task infos:\")\n",
    "\tcoco_objs = coco_instances.dataset['annotations']\n",
    "\tprint(\"Number of images: \", len(coco_instances.getImgIds()))\n",
    "\tprint(\"Number of objects: \", len(coco_objs))\n",
    "\tprint(\"Number of categories: \", len(coco_instances.cats))\n",
    "\tprint(\"Categories:\")\n",
    "\tutils.print_json(coco_instances.cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some examples from the MS COCO dataset\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Print the first image object example\n",
    "\texample_image_index = 0\n",
    "\tprint(\"\\nImage object example: \")\n",
    "\timage_example = coco_captions.loadImgs(coco_caps[example_image_index]['image_id'])[0]\n",
    "\tutils.print_json(image_example, 2)\n",
    "\n",
    "\t# Print the actual image file\n",
    "\tprint(\"\\nActual image of the example (size: \" + str(image_example['width']) + \"x\" + str(image_example['height']) + \"):\")\n",
    "\turl = image_example['coco_url']\n",
    "\timage = io.imread(url)\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(image)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Downscale the image to the maximum allowed size in the model\n",
    "\timage_max_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "\t# Crop the image to a square aspect ratio if it is not already square\n",
    "\tdownscaled_image = image\n",
    "\tif image_example['width'] > image_example['height']:\n",
    "\t\t# Image is wider than tall, crop the sides\n",
    "\t\tcrop_width = (image_example['width'] - image_example['height']) // 2\n",
    "\t\tdownscaled_image = image[:, crop_width:crop_width+image_example['height']]\n",
    "\telif image_example['height'] > image_example['width']:\n",
    "\t\t# Image is taller than wide, crop the top and bottom\n",
    "\t\tcrop_height = (image_example['height'] - image_example['width']) // 2\n",
    "\t\tdownscaled_image = image[crop_height:crop_height+image_example['width'], :]\n",
    "\t# Downscale the image to the maximum allowed size\n",
    "\tdownscaled_image = cv2.resize(downscaled_image, (image_max_size, image_max_size))\n",
    "\tprint(\"\\nDownscaled & cropped image of the example (size: \" + str(image_max_size) + \"x\" + str(image_max_size) + \"):\")\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(downscaled_image)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Print the captions for the given image\n",
    "\tprint(\"\\nCaption examples for the given image: \")\n",
    "\tcaptions_for_image = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=image_example['id']))\n",
    "\tfor caption, i in zip(captions_for_image, range(len(captions_for_image))):\n",
    "\t\tprint(str(i+1) + \") \" + caption['caption'].strip())\n",
    "\n",
    "\t# Print the captioning object example\n",
    "\tprint(\"\\nFirst caption object example:\")\n",
    "\tutils.print_json(captions_for_image[0], 2)\n",
    "\n",
    "\t# Print information about the object detection task for the given image\n",
    "\tprint(\"\\nObject detection examples for the given image:\")\n",
    "\t# Get the object detection annotations for the given image\n",
    "\tannotations_for_image = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=image_example['id']))\n",
    "\tprint(\"List of the \" + str(len(annotations_for_image)) + \" object detection annotations for the given image (obtained using the 'coco_instances.loadAnns(image_annotation_id)' function):\")\n",
    "\tfor annotation, i in zip(annotations_for_image, range(len(annotations_for_image))):\n",
    "\t\tprint(\"\\n> Annotation \" + str(i+1) + \":\")\n",
    "\t\t# Print the annotation object\n",
    "\t\tutils.print_json(annotation, 2, truncate_large_lists=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset of images for the training of the Vision Transformer model\n",
    "\n",
    "# Function that returns the list containing the images for the training of the Vision Transformer model\n",
    "def get_images_db(number_of_images, process_images=True):\n",
    "\t'''\n",
    "\t\tBuilds a list of images for the training of the Vision Transformer model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tnumber_of_images (int): The number of images to include in the dataset (search is stopped when the number of images is reached), use -1 to include all available images\n",
    "\t\t\tprocess_images (bool): Whether to process the images (i.e. retrieve actual image data, crop images and compupte their base64 encodings to add to the list)\n",
    "\t'''\n",
    "\t# Structure of the images\n",
    "\timages_list_object = {\n",
    "\t\t\"image_id\": \"\",\t\t\t# ID of the image (as found in the COCO dataset)\n",
    "\t\t\"image_url\": \"\",\t\t# URL of the image\n",
    "\t\t\"image_width\": 0,\t\t# The original image width\n",
    "\t\t\"image_height\": 0,\t\t# The original image height\n",
    "\t\t\"image_captions\": [],\t# List of captions for the image\n",
    "\t\t\"image_classes\": [\t\t# List of classes for the image (i.e. detected objects, in the order of area size)\n",
    "\t\t\t{\n",
    "\t\t\t\t\"class_id\": 0,\t\t# ID of the class (as found in the COCO dataset)\n",
    "\t\t\t\t\"class_name\": \"\",\t# Name of the class\n",
    "\t\t\t\t\"class_area\": 0,\t# Sum of the area of each instance of the class in the image\n",
    "\t\t\t\t\"class_count\": 0\t# Number of instances of the class in the image\n",
    "\t\t\t}\n",
    "\t\t],\t\n",
    "\t\t\"image_data\": \"\"\t\t# Base64 string of the image\n",
    "\t}\n",
    "\t# Get the image ids\n",
    "\timg_ids = coco_captions.getImgIds()\n",
    "\t# Get the images\n",
    "\timages = []\n",
    "\t\n",
    "\t# Function that returns a list of images with the given aspect ratio tolerance\n",
    "\tdef select_images_list(image_aspect_ratio_tolerance):\n",
    "\t\t# Get the images\n",
    "\t\tfor img_id in img_ids:\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg_obj = coco_captions.loadImgs(img_id)[0]\n",
    "\t\t\t# Check if the size of the image is square or within the aspect ratio tolerance\n",
    "\t\t\timage_aspect_ratio = img_obj['width'] / img_obj['height']\n",
    "\t\t\tif abs(image_aspect_ratio - 1) > image_aspect_ratio_tolerance:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Check if the image is already in the images list\n",
    "\t\t\tif any(img['image_id'] == img_obj['id'] for img in images):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Get the image url\n",
    "\t\t\timg_url = img_obj['coco_url']\n",
    "\t\t\t# Get the captions for the image\n",
    "\t\t\timg_captions = []\n",
    "\t\t\tcaptions = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tfor caption in captions:\n",
    "\t\t\t\tcaption_text = caption['caption'].strip()\n",
    "\t\t\t\tif len(caption_text) > 1:\n",
    "\t\t\t\t\timg_captions.append(caption_text)\n",
    "\t\t\t# Discard the image if the number of captions is less than the minimum\n",
    "\t\t\tif len(img_captions) < MIN_IMAGE_CAPTIONS:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Discard the image if it has no classes\n",
    "\t\t\tclasses = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tif len(classes) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Create a classes object with the fields: \"class_id\", \"class_name\", \"class_area\"\n",
    "\t\t\tclasses_obj = {}\n",
    "\t\t\tfor class_obj in classes:\n",
    "\t\t\t\tclass_id = class_obj['category_id']\n",
    "\t\t\t\tclass_name = coco_instances.cats[class_id]['name']\n",
    "\t\t\t\tclass_area = class_obj['area']\n",
    "\t\t\t\tif class_id not in classes_obj:\n",
    "\t\t\t\t\tclasses_obj[class_id] = {\n",
    "\t\t\t\t\t\t\"class_id\": class_id,\n",
    "\t\t\t\t\t\t\"class_name\": class_name,\n",
    "\t\t\t\t\t\t\"class_area\": class_area,\n",
    "\t\t\t\t\t\t\"class_count\": 1,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_area'] += class_area\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_count'] += 1\n",
    "\t\t\t# Convert the classes object to a list\n",
    "\t\t\tclasses_obj = list(classes_obj.values())\n",
    "\t\t\t# Sort the classes by area size\n",
    "\t\t\tclasses_obj = sorted(classes_obj, key=lambda x: x['class_area'], reverse=True)\n",
    "\t\t\t# Add the image to the images list\n",
    "\t\t\timages_list_object = {\n",
    "\t\t\t\t\"image_id\": img_obj['id'],\n",
    "\t\t\t\t\"image_url\": img_url,\n",
    "\t\t\t\t\"image_width\": img_obj['width'],\n",
    "\t\t\t\t\"image_height\": img_obj['height'],\n",
    "\t\t\t\t\"image_captions\": img_captions,\n",
    "\t\t\t\t\"image_classes\": classes_obj,\n",
    "\t\t\t\t\"image_data\": None # Will be filled later\n",
    "\t\t\t}\n",
    "\t\t\timages.append(images_list_object)\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif number_of_images >= 1 and len(images) >= number_of_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Return the images list\n",
    "\t\treturn images\n",
    "\tprint(\"Selecting images with a square aspect ratio...\")\n",
    "\t# Get the images that have a square aspect ratio first\n",
    "\timages = select_images_list(0)\n",
    "\t# Get the remaining images with the given aspect ratio tolerance\n",
    "\tif len(images) < number_of_images or number_of_images == -1:\n",
    "\t\tsquare_aspect_ratio_images = len(images)\n",
    "\t\tprint(\"> Found \" + str(square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio, looking for the remaining images...\")\n",
    "\t\tprint(\"Looking for remaining images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"% (either a \" + str(1 + MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio or a \" + str(1 - MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio)...\")\n",
    "\t\timages = select_images_list(MAX_ASPECT_RATIO_TOLERANCE)\n",
    "\t\tnon_square_aspect_ratio_images = len(images) - square_aspect_ratio_images\n",
    "\t\t# Print the number of images found\n",
    "\t\tprint(\"> Found \" + str(non_square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" )  + \" more images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"%.\")\n",
    "\telse:\n",
    "\t\tprint(\"> Found \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio.\")\n",
    "\t# Print a message based on the number of images found\n",
    "\tif len(images) < number_of_images and number_of_images != -1:\n",
    "\t\tprint(\"WARNING: Could not find enough images with the required aspect ratio tolerance, only \" + str(len(images)) + \" / \" + str(number_of_images) + \" images found.\")\n",
    "\telse:\n",
    "\t\tprint(\"DONE: Found all \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with the required aspect ratio tolerance.\")\n",
    "\t# Get all the image data\n",
    "\tif process_images:\n",
    "\t\tfor img in tqdm(images, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\t\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\t# Return the images list\n",
    "\treturn images\n",
    "\n",
    "# List of image objects used for the training of the Vision Transformer model\n",
    "images_db = []\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Check if the images list should be rebuilt or loaded\n",
    "\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\tif os.path.exists(images_db_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\t\twith open(images_db_file, 'r') as f:\n",
    "\t\t\timages_db = json.load(f)\n",
    "\t\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "\telse:\n",
    "\t\t# Initialize the images list\n",
    "\t\timages_db = get_images_db(-1, False)\n",
    "\t\t# Save the images list to a JSON file\n",
    "\t\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\t\tprint(\"Saving the images list to the file: \", images_db_file)\n",
    "\t\twith open(images_db_file, 'w') as f:\n",
    "\t\t\tjson.dump(images_db, f)\n",
    "\n",
    "\t# Print the final number of images in the dataset\n",
    "\tprint(\"\\nNumber of loaded images in the dataset: \" + str(len(images_db)) + (\"/\" + str(NUMBER_OF_IMAGES_IN_DB) if NUMBER_OF_IMAGES_IN_DB != -1 else \"\"))\n",
    "else:\n",
    "\t# Load the imaged_db.json file from the demo folder\n",
    "\timages_db_file = os.path.join(DEMO_FOLDER, \"images_db.json\")\n",
    "\tif os.path.exists(images_db_file):\n",
    "\t\twith open(images_db_file, 'r') as f:\n",
    "\t\t\timages_db = json.load(f)\n",
    "\t\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list from the file: \", images_db_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list from the file: \" + images_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"classes\" dictionary with the classes found in the dataset and, for each of them, a list of the images in which they appear\n",
    "\n",
    "# initialize the classes dictionary\n",
    "classes = {}\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Function to get the classes dictionary from the images\n",
    "\tdef get_classes_dict():\n",
    "\t\t# Initialize the classes list\n",
    "\t\tclasses = {}\n",
    "\t\t# Get the classes from the images\n",
    "\t\tfor i in tqdm(range(len(images_db)), desc=\"Processing images for classes...\"):\n",
    "\t\t\timg = images_db[i]\n",
    "\t\t\tfor class_obj in img['image_classes']:\n",
    "\t\t\t\t# Get the class id\n",
    "\t\t\t\tclass_id = class_obj['class_id']\n",
    "\t\t\t\t# Add the class to the classes list if it does not exist\n",
    "\t\t\t\tif class_id not in classes.keys():\n",
    "\t\t\t\t\tclasses[class_id] = []\n",
    "\t\t\t\t# Add the image index to the class list\n",
    "\t\t\t\tclasses[class_id].append(i)\n",
    "\t\tprint(\"Created the classes list from the images with \" + str(len(classes)) + \" classes.\")\n",
    "\t\t# Discard the classes with less than the minimum number of images\n",
    "\t\tclasses = {k: v for k, v in classes.items() if len(v) >= MIN_IMAGES_PER_CLASS}\n",
    "\t\t# Sort classes by the number of images\n",
    "\t\tclasses = {k: v for k, v in sorted(classes.items(), key=lambda item: len(item[1]), reverse=True)}\n",
    "\t\tprint(\"Discarded the classes with less than \" + str(MIN_IMAGES_PER_CLASS) + \" images: \" + str(len(classes)) + \" / \" + str(len(classes.keys()) + len(classes)) + \" classes remaining.\")\n",
    "\t\t# Return the classes list\n",
    "\t\treturn classes\n",
    "\n",
    "\t# Get the classes dictionary if it already exists, otherwise create it\n",
    "\tclasses_file = os.path.join(DATA_FOLDER, \"classes.json\")\n",
    "\tif os.path.exists(classes_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\t\twith open(classes_file, 'r') as f:\n",
    "\t\t\tclasses = json.load(f)\n",
    "\t\tif len(classes) > 0:\n",
    "\t\t\tprint(\"Loaded the classes dictionary from the file: \", classes_file)\n",
    "\telse:\n",
    "\t\tprint(\"Creating the classes dictionary from the images...\")\n",
    "\t\tclasses = get_classes_dict()\n",
    "\t\t# Save the classes dictionary to a JSON file\n",
    "\t\tprint(\"Saving the classes dictionary to the file: \", classes_file)\n",
    "\t\twith open(classes_file, 'w') as f:\n",
    "\t\t\tjson.dump(classes, f)\n",
    "else:\n",
    "\t# Load the classes.json file from the demo folder\n",
    "\tclasses_file = os.path.join(DEMO_FOLDER, \"classes.json\")\n",
    "\tif os.path.exists(classes_file):\n",
    "\t\twith open(classes_file, 'r') as f:\n",
    "\t\t\tclasses = json.load(f)\n",
    "\t\tprint(\"Loaded the classes dictionary from the file: \", classes_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo classes dictionary from the file: \", classes_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo classes dictionary from the file: \" + classes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the images in the images DB to finally only include the images that have the classes in the classes list, with MIN_IMAGES_PER_CLASS images per class, and to populate the images list with the base64 encoding of the images\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Function to update the images list to only include the images that have the classes in the classes list\n",
    "\tdef update_images_db_based_on_classes(max_images):\n",
    "\t\t# Number of classes to maintain the designated number of images\n",
    "\t\tclasses_count = math.ceil(max_images / MIN_IMAGES_PER_CLASS)\n",
    "\t\t# Initialize the new images list\n",
    "\t\tnew_images_db = []\n",
    "\t\t# Get the classes to maintain the designated number of images\n",
    "\t\tclasses_to_maintain = list(classes.keys())[:classes_count]\n",
    "\t\t# Create a new classes list with the classes found in the new images list\n",
    "\t\tnew_classes = {}\n",
    "\t\t# Get the images to maintain the designated number of images\n",
    "\t\tfor i in tqdm(range(len(images_db)), desc=\"Processing images for classes...\"):\n",
    "\t\t\timg = images_db[i]\n",
    "\t\t\t# Check if the image has any of the classes to maintain\n",
    "\t\t\tif any(class_obj['class_id'] in classes_to_maintain for class_obj in img['image_classes']):\n",
    "\t\t\t\tnew_images_db.append(img)\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif len(new_images_db) >= max_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Get the classes from the classes to maintain\n",
    "\t\tfor class_id in classes_to_maintain:\n",
    "\t\t\t# Remove any image index that is not in the new images list\n",
    "\t\t\tnew_classes[class_id] = [i for i in classes[class_id] if i < len(new_images_db)]\n",
    "\t\t# Sort the classes by the number of images\n",
    "\t\tnew_classes = { k: v for k, v in sorted(new_classes.items(), key=lambda item: len(item[1]), reverse=True) }\n",
    "\t\t# Return the new images list\n",
    "\t\treturn new_images_db, new_classes\n",
    "\n",
    "\t# Update the images list to only include the images that have the classes in the classes list\n",
    "\tmax_images = NUMBER_OF_IMAGES_IN_DB if NUMBER_OF_IMAGES_IN_DB != -1 else len(images_db)\n",
    "\tprint(\"\\nUpdating the images list to only include the \" + str(len(classes)) + \" classes with at least \" + str(MIN_IMAGES_PER_CLASS) + \" images, not exceeding \" + str(max_images) + \" images...\")\n",
    "\timages_db, classes = update_images_db_based_on_classes(max_images)\n",
    "\tprint(\"DONE: Updated the images list, now containing \" + str(len(images_db)) + \" images.\")\n",
    "\tprint(\"> Final number of classes in the dataset: \" + str(len(classes)))\n",
    "\n",
    "\t# Update the images list to include the base64 encoding of the images\n",
    "\tprint(\"Computing the BASE64 images encoding for the images list...\")\n",
    "\tfor img in tqdm(images_db, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\tprint(\"DONE: Computed the BASE64 images encoding for the images list.\")\n",
    "\n",
    "\t# Save the updated images list to a JSON file\n",
    "\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\tprint(\"Saving the updated images list to the file: \", images_db_file)\n",
    "\twith open(images_db_file, 'w') as f:\n",
    "\t\tjson.dump(images_db, f)\n",
    "\n",
    "# Print the final number of images in the dataset\n",
    "print(\"\\nNumber of loaded images in the dataset: \" + str(len(images_db)))\n",
    "print(\"Number of classes in the dataset: \" + str(len(classes)))\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first image object example\n",
    "example_image_index = -1\n",
    "print(\"Image object example: \")\n",
    "utils.print_json(images_db[example_image_index], 2)\n",
    "\n",
    "# Print the actual image file\n",
    "image_b64_string = images_db[example_image_index]['image_data'] if images_db[example_image_index]['image_data'] != None else utils.get_image_data_as_base64(images_db[example_image_index]['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "image = utils.get_image_from_b64_string(image_b64_string)\n",
    "print(\"\\nActual image of the example (original size: \" + str(images_db[example_image_index]['image_width']) + \"x\" + str(images_db[example_image_index]['image_height']) + \" | downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Print how the Transformer model sees the image\n",
    "print(\"\\nHow the Transformer model sees the image (downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "# Divide the image into smaller images representing the patches\n",
    "image_patches = []\n",
    "for i in range(0, image.shape[0], IMAGE_PATCH_SIZE):\n",
    "\tfor j in range(0, image.shape[1], IMAGE_PATCH_SIZE):\n",
    "\t\timage_patch = image[i:i+IMAGE_PATCH_SIZE, j:j+IMAGE_PATCH_SIZE]\n",
    "\t\timage_patches.append(image_patch)\n",
    "# Display the image patches\n",
    "fig, axs = plt.subplots(IMAGE_PATCHES_PER_DIMENSION, IMAGE_PATCHES_PER_DIMENSION, figsize=(10, 10))\n",
    "for i in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\tfor j in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\t\taxs[i, j].imshow(image_patches[i*IMAGE_PATCHES_PER_DIMENSION+j])\n",
    "\t\taxs[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classes list\n",
    "total_classes = -1 if USE_DEMO_DATA else len(coco_instances.cats)\n",
    "print(\"\\nClasses list sorted by number of images (\" + str(len(classes)) + \" classes out of \" + (str(total_classes) if total_classes != -1 else \"???\") + \" total MS COCO classes):\")\n",
    "utils.print_json(classes, 2, truncate_large_lists=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the images list into a list for the indexing dataset (i.e. images in the database) and a list for the image retrieval dataset (i.e. similar images to retrieve images in the DB)\n",
    "\n",
    "# List of images for the indexing and image retrieval datasets\n",
    "images_db_indexing = []\t# List of images for the indexing dataset\n",
    "images_db_image_retrieval = {} # Dictionary containing image IDs of images NOT in the indexing dataset as keys and the list of similar images in the indexing dataset as values\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Create the indexing and image retrieval datasets from the images list\n",
    "\tfor class_id in classes.keys():\n",
    "\t\tclass_obj = classes[class_id]\n",
    "\t\tindexing_number = int(len(class_obj) * (1 - IMAGE_RETRIEVAL_DB_PERCENTAGE))\n",
    "\t\tsimilar_images = []\n",
    "\t\tfor i in range(len(class_obj)):\n",
    "\t\t\tis_in_db = i < indexing_number\n",
    "\t\t\timg_id = class_obj[i]\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg = images_db[img_id]\n",
    "\t\t\tif is_in_db:\n",
    "\t\t\t\t# Add the image to the indexing dataset\n",
    "\t\t\t\timages_db_indexing.append(img)\n",
    "\t\t\t\t# Add the image to the similar images list\n",
    "\t\t\t\tsimilar_images.append(img_id)\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Add the image to the image retrieval dataset\n",
    "\t\t\t\timages_db_image_retrieval[img_id] = similar_images\n",
    "\t# Save the indexing and image retrieval datasets to JSON files\n",
    "\timages_db_indexing_file = os.path.join(DATA_FOLDER, \"images_db_indexing.json\")\n",
    "\tprint(\"Saving the images list for the indexing dataset to the file: \", images_db_indexing_file)\n",
    "\twith open(images_db_indexing_file, 'w') as f:\n",
    "\t\tjson.dump(images_db_indexing, f)\n",
    "\timages_db_image_retrieval_file = os.path.join(DATA_FOLDER, \"images_db_image_retrieval.json\")\n",
    "\tprint(\"Saving the images list for the image retrieval dataset to the file: \", images_db_image_retrieval_file)\n",
    "\twith open(images_db_image_retrieval_file, 'w') as f:\n",
    "\t\tjson.dump(images_db_image_retrieval, f)\n",
    "else:\n",
    "\t# Load the images_db_indexing.json and images_db_image_retrieval.json files from the demo folder\n",
    "\timages_db_indexing_file = os.path.join(DEMO_FOLDER, \"images_db_indexing.json\")\n",
    "\timages_db_image_retrieval_file = os.path.join(DEMO_FOLDER, \"images_db_image_retrieval.json\")\n",
    "\tif os.path.exists(images_db_indexing_file):\n",
    "\t\twith open(images_db_indexing_file, 'r') as f:\n",
    "\t\t\timages_db_indexing = json.load(f)\n",
    "\t\tprint(\"Loaded the images list for the indexing dataset from the file: \", images_db_indexing_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list for the indexing dataset from the file: \", images_db_indexing_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list for the indexing dataset from the file: \" + images_db_indexing_file)\n",
    "\tif os.path.exists(images_db_image_retrieval_file):\n",
    "\t\twith open(images_db_image_retrieval_file, 'r') as f:\n",
    "\t\t\timages_db_image_retrieval = json.load(f)\n",
    "\t\tprint(\"Loaded the images list for the image retrieval dataset from the file: \", images_db_image_retrieval_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list for the image retrieval dataset from the file: \", images_db_image_retrieval_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list for the image retrieval dataset from the file: \" + images_db_image_retrieval_file)\n",
    "\n",
    "# Compute the max length of the image IDS (we consider the index of the image in the \"images_db\" as the image ID)\n",
    "max_image_id_length = len(str(len(images_db_indexing)))\n",
    "# Number of output tokens for the encoded image IDs (the 10 digits [0-9] plus the 3 special tokens, i.e. end of sequence, padding, start of sequence)\n",
    "output_tokens = 10 + 3\n",
    "\n",
    "# Print the final number of images in the datasets\n",
    "print(\"\\nNumber of images in the indexing dataset: \" + str(len(images_db_indexing)))\n",
    "print(\"Image IDs max length: \" + str(max_image_id_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of the file in which the PyTorch datasets will be stored or from which they will be loaded\n",
    "transformer_indexing_dataset_file = os.path.join(\n",
    "\tDATA_FOLDER if not USE_DEMO_DATA else DEMO_FOLDER,\n",
    "\t\"transformer_indexing_dataset.json\"\n",
    ")\n",
    "transformer_image_retrieval_dataset_file = os.path.join(\n",
    "\tDATA_FOLDER if not USE_DEMO_DATA else DEMO_FOLDER,\n",
    "\t\"transformer_image_retrieval_dataset.json\"\n",
    ")\n",
    "\n",
    "# Build the Transformer Indexing Database for training the vision transformer\n",
    "transformer_indexing_dataset = datasets.TransformerIndexingDataset(\n",
    "\timages=images_db_indexing,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=IMAGE_PATCHES_PER_DIMENSION,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=transformer_indexing_dataset_file,\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION\n",
    ")\n",
    "\n",
    "# Build the Transformer Image Retrieval Database for training the vision transformer\n",
    "transformer_image_retrieval_dataset = datasets.TransformerImageRetrievalDataset(\n",
    "\tall_images=images_db,\n",
    "\tsimilar_images=images_db_image_retrieval,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=IMAGE_PATCHES_PER_DIMENSION,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=transformer_image_retrieval_dataset_file,\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first example from the Transformer Indexing Dataset\n",
    "example_index = random.randint(0, len(transformer_indexing_dataset)-1)\n",
    "print(\"Example from the Transformer Indexing Dataset:\")\n",
    "print(\"<encoded_image, encoded_image_id> tuple:\")\n",
    "print(transformer_indexing_dataset[example_index])\n",
    "\n",
    "# Print the first example from the Transformer Image Retrieval Dataset\n",
    "example_index = random.randint(0, len(transformer_image_retrieval_dataset)-1)\n",
    "print(\"\\nExample from the Transformer Image Retrieval Dataset:\")\n",
    "print(\"<encoded_image, encoded_similar_image_if> tuple:\")\n",
    "print(transformer_image_retrieval_dataset[example_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_EMBEDDINGS_SIZE = 128\n",
    "\n",
    "TRANSFORMER_INDEXING_TRAINING_EPOCHS = 250\n",
    "TRANSFORMER_RETRIEVAL_TRAINING_EPOCHS = 150\n",
    "\n",
    "def train_and_evaluate_transformer():\n",
    "\t''' Auxiliary function to train (or load checkpoints), show training results, and evaluate the transformer model of the given type '''\n",
    "\t\n",
    "\tdsi_transformer_args = {\n",
    "\t\t# Dimensionality of the input feature vectors to the Transformer (i.e. the size of the embeddings)\n",
    "\t\t\"embed_dim\": TRANSFORMER_EMBEDDINGS_SIZE, \n",
    "\t\t# Dimensionality of the hidden layer in the feed-forward networks within the Transformer\n",
    "\t\t\"hidden_dim\": 256, \n",
    "\t\t# Number of channels of the input (e.g. 3 for RGB, 1 for grayscale, ecc...)\n",
    "\t\t\"num_channels\": 3,\t\n",
    "\t\t# Number of heads to use in the Multi-Head Attention block\n",
    "\t\t\"num_heads\": 4,\t\n",
    "\t\t# Number of layers to use in the Transformer\n",
    "\t\t\"num_layers\": 3,\n",
    "\t\t# Size of each batch\n",
    "\t\t\"batch_size\": 32,\n",
    "\t\t# Number of classes to predict (in my case, since I give an image with, concatenated, the N digits of the image ID, the num_classes is the number of possible digits of the image IDs, hence 10+3, including the special tokens)\n",
    "\t\t\"num_classes\": output_tokens,\n",
    "\t\t# Size of the image patches\n",
    "\t\t\"patch_size\": IMAGE_PATCH_SIZE,\n",
    "\t\t# Maximum number of patches an image can have\n",
    "\t\t\"num_patches\": IMAGE_PATCHES_PER_DIMENSION * IMAGE_PATCHES_PER_DIMENSION,\n",
    "\t\t# Maximum length of the image IDs\n",
    "\t\t\"img_id_max_length\": max_image_id_length,\n",
    "\t\t# Special tokens for the image IDs\n",
    "\t\t\"img_id_start_token\": 10,\n",
    "\t\t\"img_id_end_token\": 12,\n",
    "\t\t\"img_id_padding_token\": 11,\n",
    "\t\t# Dropout to apply in the feed-forward network and on the input encoding\n",
    "\t\t\"dropout\": 0.2,\n",
    "\t}\n",
    "\n",
    "\t# Initialize transformer model (using scheduled sampling)\n",
    "\ttransformer_model = models.DSI_VisionTransformer(**dsi_transformer_args)\n",
    "\n",
    "\t# Model's type string\n",
    "\tmodel_type_string = \"DSI_VisionTransformer\"\n",
    "\n",
    "\t# Model's checkpoint file\n",
    "\tmodel_checkpoint_file = MODELS_FOLDER + \"/\" + model_type_string + \"_\" + MODEL_CHECKPOINT_FILE\n",
    "\n",
    "\t# Train the model or load its saved checkpoint\n",
    "\ttransformer_retrieval_test_set = None\n",
    "\ttransformer_retrieval_test_set_file = DATA_FOLDER + f\"/{model_type_string}_transformer_retrieval_test_set.json\"\n",
    "\tif LOAD_MODELS_CHECKPOINTS and os.path.exists(model_checkpoint_file):\n",
    "\t\t# Load the saved models checkpoint\n",
    "\t\tprint(\"A checkpoint for the model exist, loading the saved model checkpoint...\")\n",
    "\t\ttransformer_model = models.DSI_VisionTransformer.load_from_checkpoint(model_checkpoint_file, **dsi_transformer_args)\n",
    "\t\tprint(\"Model checkpoint loaded.\")\n",
    "\t\t# Load the transformer retrieval test set from the JSON file\n",
    "\t\tprint(\"Loading the transformer retrieval test set from the JSON file...\")\n",
    "\t\twith open(transformer_retrieval_test_set_file, \"r\") as transformer_retrieval_test_set_file:\n",
    "\t\t\ttransformer_retrieval_test_set = json.load(transformer_retrieval_test_set_file)\n",
    "\t\tprint(\"Transformer retrieval test set loaded.\")\n",
    "\telse:\n",
    "\t\t# Create 2 loggers for the transformer model (one for the indexing task and one for the retrieval task)\n",
    "\t\ttransformer_loggers = None\n",
    "\t\tif wandb_api is not None:\n",
    "\t\t\ttransformer_wandb_logger_indexing = WandbLogger(log_model=\"all\", project=wandb_project, name=model_type_string + \" (Indexing)\")\n",
    "\t\t\ttransformer_wandb_logger_retrieval = WandbLogger(log_model=\"all\", project=wandb_project, name=model_type_string + \" (Retrieval)\")\n",
    "\t\t\ttransformer_loggers = [transformer_wandb_logger_indexing, transformer_wandb_logger_retrieval]\n",
    "\t\t# Train the transformer model (with scheduled sampling) for the indexing task\n",
    "\t\ttransformer_training_infos = training.train_transformer(\n",
    "\t\t\ttransformer_indexing_dataset=transformer_indexing_dataset,\n",
    "\t\t\ttransformer_retrieval_dataset=transformer_image_retrieval_dataset,\n",
    "\t\t\ttransformer_model=transformer_model,\n",
    "\t\t\tmax_epochs_list=[TRANSFORMER_INDEXING_TRAINING_EPOCHS, TRANSFORMER_RETRIEVAL_TRAINING_EPOCHS],\n",
    "\t\t\tbatch_size=transformer_model.hparams.batch_size,\n",
    "\t\t\tindexing_split_ratios=(1.0, 0.0),\n",
    "\t\t\tretrieval_split_ratios=(0.9, 0.05, 0.05),\n",
    "\t\t\tlogger=transformer_loggers,\n",
    "\t\t\tsave_path=model_checkpoint_file\n",
    "\t\t)\n",
    "\t\t# Show the wandb training run's dashboard\n",
    "\t\tif wandb_api is not None:\n",
    "\t\t\tindexing_run_id = transformer_training_infos[\"run_ids\"][\"indexing\"]\n",
    "\t\t\tif indexing_run_id is not None:\n",
    "\t\t\t\tprint(f\"Indexing training results for the {model_type_string} model:\")\n",
    "\t\t\t\tindexing_run_object: wandb_run.Run = wandb_api.run(f\"{wandb_entity}/{wandb_project}/{indexing_run_id}\")\n",
    "\t\t\t\tindexing_run_object.display(height=1000)\n",
    "\t\t\tretrieval_run_id = transformer_training_infos[\"run_ids\"][\"retrieval\"]\n",
    "\t\t\tif retrieval_run_id is not None:\n",
    "\t\t\t\tprint(f\"Retrieval training results for the {model_type_string} model:\")\n",
    "\t\t\t\tretrieval_run_object: wandb_run.Run = wandb_api.run(f\"{wandb_entity}/{wandb_project}/{retrieval_run_id}\")\n",
    "\t\t\t\tretrieval_run_object.display(height=1000)\n",
    "\t\t# Save the generated transformer retrieval test set to the JSON file\n",
    "\t\tprint(\"Saving the transformer retrieval test set to the JSON file...\")\n",
    "\t\tretrieval_test_dataset = transformer_training_infos[\"retrieval\"][\"test\"]\n",
    "\t\ttransformer_retrieval_test_set = {\n",
    "\t\t\t\"encoded_queries\": [],\n",
    "\t\t\t\"encoded_doc_ids\": []\n",
    "\t\t}\n",
    "\t\tretrieval_test_dataset_length = retrieval_test_dataset.__len__()\n",
    "\t\tfor i in range(retrieval_test_dataset_length):\n",
    "\t\t\tencoded_query, doc_id = retrieval_test_dataset.__getitem__(i)\n",
    "\t\t\ttransformer_retrieval_test_set[\"encoded_queries\"].append(encoded_query.tolist())\n",
    "\t\t\ttransformer_retrieval_test_set[\"encoded_doc_ids\"].append(doc_id.tolist())\n",
    "\t\twith open(transformer_retrieval_test_set_file, \"w\") as transformer_retrieval_test_set_file:\n",
    "\t\t\tjson.dump(transformer_retrieval_test_set, transformer_retrieval_test_set_file)\n",
    "\n",
    "\t'''\n",
    "\t# Evaluate the transformer model (for the retrieval task)\n",
    "\tif EVALUATE_MODELS:\n",
    "\t\ttransformer_retrieval_map_k = evaluation.compute_mean_average_precision_at_k(\n",
    "\t\t\tMODEL_TYPES.DSI_TRANSFORMER, queries_dict, docs_dict,\n",
    "\t\t\tk_documents=MAP_K, n_queries=MAP_N,\n",
    "\t\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\t\t# Keyword arguments for the Transformer model\n",
    "\t\t\tmodel=transformer_model, retrieval_dataset=transformer_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t\t)\n",
    "\t\ttransformer_retrieval_recall_k = evaluation.compute_recall_at_k(\n",
    "\t\t\tMODEL_TYPES.DSI_TRANSFORMER, queries_dict, docs_dict,\n",
    "\t\t\tk_documents=RECALL_K,\n",
    "\t\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\t\t# Keyword arguments for the Transformer model\n",
    "\t\t\tmodel=transformer_model, retrieval_dataset=transformer_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t\t)\n",
    "\t\tprint_model_evaluation_results(transformer_retrieval_map_k, transformer_retrieval_recall_k)\n",
    "\t'''\n",
    "\n",
    "\t# return transformer_model, transformer_retrieval_map_k, transformer_retrieval_recall_k\n",
    "\treturn transformer_model, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the vision transformer model\n",
    "teacher_forcing_transformer, teacher_forcing_transformer_map_k, teacher_forcing_transformer_recall_k = train_and_evaluate_transformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
