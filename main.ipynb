{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on colab or locally\n",
    "try:\n",
    "\tfrom google.colab import files\n",
    "\tRUNNING_IN_COLAB = True\n",
    "\tprint(\"Running on Google Colab.\")\n",
    "except ModuleNotFoundError:\n",
    "\tRUNNING_IN_COLAB = False\n",
    "\tprint(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# Clone the git repository of the project for the source files\n",
    "!git clone https://github.com/valeriodiste/computer_vision_project_dev.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# Change the working directory to the cloned repository\n",
    "# TO DO: change the directory to the correct one\n",
    "%cd /content/computer_vision_project_dev\n",
    "# Pull the latest changes from the repository\n",
    "!git pull origin main\n",
    "# Change the working directory to the parent directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# Install the required packages\n",
    "# %%capture\n",
    "%pip install pytorch-lightning\n",
    "%pip install pycocotools\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the standard libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Import the PyTorch libraries and modules\n",
    "import torch\n",
    "\n",
    "# Import the PyTorch Lightning libraries and modules\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import the coco library\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Import the W&B (Weights & Biases) library\n",
    "# import wandb\n",
    "# from wandb.sdk import wandb_run\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import cv2\n",
    "import base64\n",
    "\n",
    "# Import the tqdm library (for the progress bars)\n",
    "if not RUNNING_IN_COLAB:\n",
    "\tfrom tqdm import tqdm\n",
    "else:\n",
    "\tfrom tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb.sdk' has no attribute 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import the custom modules\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m RUNNING_IN_COLAB:\n\u001b[0;32m      3\u001b[0m \t\u001b[38;5;66;03m# We are running locally (not on Google Colab, import modules from the \"src\" directory in the current directory)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \t\u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscripts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, datasets, training, evaluation, utils\t\u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t\u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m \t\u001b[38;5;66;03m# We are running on Google Colab (import modules from the pulled repository stored in the project's directory)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\valer\\Desktop\\computer_vision_project_dev\\src\\scripts\\training.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Import custom modules\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wandb\\__init__.py:31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m \u001b[43mwandb_sdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     33\u001b[0m init \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39minit\n\u001b[0;32m     34\u001b[0m setup \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39msetup\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'wandb.sdk' has no attribute 'lib'"
     ]
    }
   ],
   "source": [
    "# Import the custom modules\n",
    "if not RUNNING_IN_COLAB:\n",
    "\t# We are running locally (not on Google Colab, import modules from the \"src\" directory in the current directory)\n",
    "\tfrom src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore\n",
    "else:\n",
    "\t# We are running on Google Colab (import modules from the pulled repository stored in the project's directory)\n",
    "\tfrom computer_vision_project_dev.src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom computer_vision_project_dev.src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "\n",
    "# ===== Training & Datasets constants =====================================================\n",
    "\n",
    "# MS COCO dataset constants (use MS COCO 2014 dataset for image captioning)\n",
    "COCO_DATA_YEAR = '2014'  \t# '2014' or '2017'\n",
    "COCO_DATA_TYPE = 'val'  # 'train' or 'val'\n",
    "COCO_DATA_CAPTIONS_FILE = f\"/annotations/captions_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the annotations file inside the DATA_FOLDER\n",
    "CODO_DATA_INSTANCES_FILE = f\"/annotations/instances_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the instances file inside the DATA_FOLDER\n",
    "\n",
    "# Size of the image patches\n",
    "IMAGE_PATCH_SIZE = 16\n",
    "# Number of image patches per dimension (i.e. both vertically and horizontally, since images have a square aspect ratio)\n",
    "IMAGE_PATCHES_PER_DIMENSION = 10\t# 3x3 patches, 48x48 pixels images\n",
    "\n",
    "# Total number of images to consider in the dataset (will be split into training, validation and test sets), set to -1 to use all the available images\n",
    "NUMBER_OF_IMAGES_IN_DB = 100\t\t# Was 1000\n",
    "# Minimum number of captions for an image\n",
    "MIN_IMAGE_CAPTIONS = 5\n",
    "# If not enough square images are found, also accept images that have this max aspect difference (they will be cropped to a square aspect ratio later)\n",
    "MAX_ASPECT_RATIO_TOLERANCE = 0.1 \t# Accept images that are 10% wider than they are tall (or vice versa)\n",
    "# Minimum number of images per class\n",
    "MIN_IMAGES_PER_CLASS = 10\t# Was 100\n",
    "\n",
    "# Percentage of images, for each class, to use for the image retrieval dataset (the remaining images will be used for the indexing dataset, i.e. will be added in the images database)\n",
    "IMAGE_RETRIEVAL_DB_PERCENTAGE = 0.8\n",
    "\n",
    "# ===== Evaluation constants ==============================================================\n",
    "\n",
    "# Define the number of images K to retrieve for each query and the number of queries N to calculate the mean average precision (MAP@K)\n",
    "MAP_K = 10\n",
    "MAP_N = 10\n",
    "\n",
    "# Define the number of images K to retrieve for each query to calculate the Recall@K metrics\n",
    "RECALL_K = 1_000\n",
    "\n",
    "# Whether to print the debug information during the MAP@K and Recall@K evaluation of the models\n",
    "PRINT_EVALUATION_DEBUG = True\n",
    "\n",
    "# Whether to evaluate the models (i.e. compute the MAP@K and Recall@K metrics for the trained models on the test datasets)\n",
    "EVALUATE_MODELS = True\n",
    "\n",
    "# ===== MAIN CONSTANTS =====================================================================\n",
    "\n",
    "# Whether to print examples of the images and captions during the dataset creation\n",
    "PRINT_EXAMPLES = False\n",
    "\n",
    "# Whether to load demo data from the \"demo/\" folder (set to False to build the dataset from the COCO dataset using the above constants or to load its existing version)\n",
    "USE_DEMO_DATA = True\n",
    "\n",
    "# Number of images in the demo dataset (if LOAD_DEMO_DATA is set to True)\n",
    "DEMO_DATA_SIZE = 100\t# Data will be found in the \"demo/{DEMO_DATA_SIZE}/\" folder\n",
    "\n",
    "# Define the data folder, onto which the various dictionaries, lists and other data will be saved\n",
    "DATA_FOLDER = \"src/data\" if not RUNNING_IN_COLAB else \"/content/data\"\n",
    "\n",
    "# Define the path to save models\n",
    "MODELS_FOLDER = \"src/models\" if not RUNNING_IN_COLAB else \"/content/models\"\n",
    "\n",
    "# Folder containing the demo images and captions\n",
    "DEMO_FOLDER = f\"src/demo/{DEMO_DATA_SIZE}/\" if not RUNNING_IN_COLAB else f\"/content/computer_vision_project_dev/src/demo/{DEMO_DATA_SIZE}/\"\n",
    "\n",
    "# Force the creation of the \"image_db\" images list, the JSON files for the datasets, ecc...\n",
    "FORCE_DICTIONARIES_CREATION = True\t\t# Set to false to try to load the dictionaries from the DATA_FOLDER if they exist\n",
    "\n",
    "# Whether to load model checkpoints (if they were already saved locally) or not\n",
    "LOAD_MODELS_CHECKPOINTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WANDB_API_KEY (set to \"\" to disable W&B logging)\n",
    "# NOTE: leaving the WANDB_API_KEY to a value of None will throw an error\n",
    "WANDB_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No W&B API key provided, logging with W&B disabled.\n"
     ]
    }
   ],
   "source": [
    "# Define the wandb logger, api object, entity name and project name\n",
    "wandb_logger = None\n",
    "wandb_api = None\n",
    "wandb_entity = None\n",
    "wandb_project = None\n",
    "# Check if a W&B api key is provided\n",
    "if WANDB_API_KEY == None:\n",
    "\tprint(\"No W&B API key provided, please provide a valid key to use the W&B API or set the WANDB_API_KEY variable to an empty string to disable logging\")\n",
    "\traise ValueError(\"No W&B API key provided...\")\n",
    "elif WANDB_API_KEY != \"\":\n",
    "\t# Login to the W&B (Weights & Biases) API\n",
    "\twandb.login(key=WANDB_API_KEY, relogin=True)\n",
    "\t# Minimize the logging from the W&B (Weights & Biases) library\n",
    "\tos.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\tlogging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "\t# Initialize the W&B (Weights & Biases) loggger\n",
    "\twandb_logger = WandbLogger(\n",
    "\t\tlog_model=\"all\", project=\"cv-dsi-project\", name=\"- SEPARATOR -\")\n",
    "\t# Initialize the W&B (Weights & Biases) API\n",
    "\twandb_api = wandb.Api()\n",
    "\t# Get the W&B (Weights & Biases) entity name\n",
    "\twandb_entity = wandb_logger.experiment.entity\n",
    "\t# Get the W&B (Weights & Biases) project name\n",
    "\twandb_project = wandb_logger.experiment.project\n",
    "\t# Finish the \"separator\" experiment\n",
    "\twandb_logger.experiment.finish(quiet=True)\n",
    "\tprint(\"W&B API key provided, logging with W&B enabled.\")\n",
    "else:\n",
    "\tprint(\"No W&B API key provided, logging with W&B disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they do not exist\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "\tprint(f\"Creating the data folder at '{DATA_FOLDER}'...\")\n",
    "\tos.makedirs(DATA_FOLDER)\n",
    "if not os.path.exists(MODELS_FOLDER):\n",
    "\tprint(f\"Creating the models folder at '{MODELS_FOLDER}'...\")\n",
    "\tos.makedirs(MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" non � riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n",
      "\"unzip\" non � riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n"
     ]
    }
   ],
   "source": [
    "# Check if the annotation file for the COCO dataset exists, if it does not exist, download it\n",
    "!cd {DATA_FOLDER} && wget -nc http://images.cocodataset.org/annotations/annotations_trainval{COCO_DATA_YEAR}.zip\n",
    "!cd {DATA_FOLDER} && unzip -n annotations_trainval{COCO_DATA_YEAR}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset or load the demo dataset\n",
    "coco_captions = None\n",
    "coco_instances = None\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\n",
    "\t# Initialize the COCO api for captioning\n",
    "\tcoco_captions = COCO(f\"{DATA_FOLDER}{COCO_DATA_CAPTIONS_FILE}\")\n",
    "\t# Initialize the COCO api for object detection\n",
    "\tcoco_instances = COCO(f\"{DATA_FOLDER}{CODO_DATA_INSTANCES_FILE}\")\n",
    "\n",
    "\t# Show the COCO dataset info for the captioning task\n",
    "\tprint(\"\\nCOCO captioning dataset infos:\")\n",
    "\tcoco_captions.info()\n",
    "\n",
    "\t# Show the information for the captioning task\n",
    "\tprint(\"\\nCOCO captioning task infos:\")\n",
    "\tcoco_caps = coco_captions.dataset['annotations']\n",
    "\tprint(\"Number of images: \", len(coco_captions.getImgIds()))\n",
    "\tprint(\"Number of captions: \", len(coco_caps))\n",
    "\tprint(\"Number of average captions per image: \", len(coco_caps) / len(coco_captions.getImgIds()))\n",
    "\n",
    "\t# Show the COCO dataset info for the object detection task\n",
    "\tprint(\"\\nCOCO object detection dataset infos:\")\n",
    "\tcoco_instances.info()\n",
    "\n",
    "\t# Show the information for the object detection task\n",
    "\tprint(\"\\nCOCO object detection task infos:\")\n",
    "\tcoco_objs = coco_instances.dataset['annotations']\n",
    "\tprint(\"Number of images: \", len(coco_instances.getImgIds()))\n",
    "\tprint(\"Number of objects: \", len(coco_objs))\n",
    "\tprint(\"Number of categories: \", len(coco_instances.cats))\n",
    "\tprint(\"Categories:\")\n",
    "\tutils.print_json(coco_instances.cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some examples from the MS COCO dataset\n",
    "\n",
    "if not USE_DEMO_DATA and PRINT_EXAMPLES:\n",
    "\t# Print the first image object example\n",
    "\texample_image_index = 0\n",
    "\tprint(\"\\nImage object example: \")\n",
    "\timage_example = coco_captions.loadImgs(coco_caps[example_image_index]['image_id'])[0]\n",
    "\tutils.print_json(image_example, 2)\n",
    "\n",
    "\t# Print the actual image file\n",
    "\tprint(\"\\nActual image of the example (size: \" + str(image_example['width']) + \"x\" + str(image_example['height']) + \"):\")\n",
    "\turl = image_example['coco_url']\n",
    "\timage = io.imread(url)\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(image)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Downscale the image to the maximum allowed size in the model\n",
    "\timage_max_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "\t# Crop the image to a square aspect ratio if it is not already square\n",
    "\tdownscaled_image = image\n",
    "\tif image_example['width'] > image_example['height']:\n",
    "\t\t# Image is wider than tall, crop the sides\n",
    "\t\tcrop_width = (image_example['width'] - image_example['height']) // 2\n",
    "\t\tdownscaled_image = image[:, crop_width:crop_width+image_example['height']]\n",
    "\telif image_example['height'] > image_example['width']:\n",
    "\t\t# Image is taller than wide, crop the top and bottom\n",
    "\t\tcrop_height = (image_example['height'] - image_example['width']) // 2\n",
    "\t\tdownscaled_image = image[crop_height:crop_height+image_example['width'], :]\n",
    "\t# Downscale the image to the maximum allowed size\n",
    "\tdownscaled_image = cv2.resize(downscaled_image, (image_max_size, image_max_size))\n",
    "\tprint(\"\\nDownscaled & cropped image of the example (size: \" + str(image_max_size) + \"x\" + str(image_max_size) + \"):\")\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(downscaled_image)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Print the captions for the given image\n",
    "\tprint(\"\\nCaption examples for the given image: \")\n",
    "\tcaptions_for_image = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=image_example['id']))\n",
    "\tfor caption, i in zip(captions_for_image, range(len(captions_for_image))):\n",
    "\t\tprint(str(i+1) + \") \" + caption['caption'].strip())\n",
    "\n",
    "\t# Print the captioning object example\n",
    "\tprint(\"\\nFirst caption object example:\")\n",
    "\tutils.print_json(captions_for_image[0], 2)\n",
    "\n",
    "\t# Print information about the object detection task for the given image\n",
    "\tprint(\"\\nObject detection examples for the given image:\")\n",
    "\t# Get the object detection annotations for the given image\n",
    "\tannotations_for_image = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=image_example['id']))\n",
    "\tprint(\"List of the \" + str(len(annotations_for_image)) + \" object detection annotations for the given image (obtained using the 'coco_instances.loadAnns(image_annotation_id)' function):\")\n",
    "\tfor annotation, i in zip(annotations_for_image, range(len(annotations_for_image))):\n",
    "\t\tprint(\"\\n> Annotation \" + str(i+1) + \":\")\n",
    "\t\t# Print the annotation object\n",
    "\t\tutils.print_json(annotation, 2, truncate_large_lists=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images list from the file:  src/demo/100/images_db.json\n"
     ]
    }
   ],
   "source": [
    "# Build a dataset of images for the training of the Vision Transformer model\n",
    "\n",
    "# Function that returns the list containing the images for the training of the Vision Transformer model\n",
    "def get_images_db(number_of_images, process_images=True):\n",
    "\t'''\n",
    "\t\tBuilds a list of images for the training of the Vision Transformer model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tnumber_of_images (int): The number of images to include in the dataset (search is stopped when the number of images is reached), use -1 to include all available images\n",
    "\t\t\tprocess_images (bool): Whether to process the images (i.e. retrieve actual image data, crop images and compupte their base64 encodings to add to the list)\n",
    "\t'''\n",
    "\t# Structure of the images\n",
    "\timages_list_object = {\n",
    "\t\t\"image_id\": \"\",\t\t\t# ID of the image (as found in the COCO dataset)\n",
    "\t\t\"image_url\": \"\",\t\t# URL of the image\n",
    "\t\t\"image_width\": 0,\t\t# The original image width\n",
    "\t\t\"image_height\": 0,\t\t# The original image height\n",
    "\t\t\"image_captions\": [],\t# List of captions for the image\n",
    "\t\t\"image_classes\": [\t\t# List of classes for the image (i.e. detected objects, in the order of area size)\n",
    "\t\t\t{\n",
    "\t\t\t\t\"class_id\": 0,\t\t# ID of the class (as found in the COCO dataset)\n",
    "\t\t\t\t\"class_name\": \"\",\t# Name of the class\n",
    "\t\t\t\t\"class_area\": 0,\t# Sum of the area of each instance of the class in the image\n",
    "\t\t\t\t\"class_count\": 0\t# Number of instances of the class in the image\n",
    "\t\t\t}\n",
    "\t\t],\t\n",
    "\t\t\"image_data\": \"\"\t\t# Base64 string of the image\n",
    "\t}\n",
    "\t# Get the image ids\n",
    "\timg_ids = coco_captions.getImgIds()\n",
    "\t# Get the images\n",
    "\timages = []\n",
    "\t\n",
    "\t# Function that returns a list of images with the given aspect ratio tolerance\n",
    "\tdef select_images_list(image_aspect_ratio_tolerance):\n",
    "\t\t# Get the images\n",
    "\t\tfor img_id in img_ids:\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg_obj = coco_captions.loadImgs(img_id)[0]\n",
    "\t\t\t# Check if the size of the image is square or within the aspect ratio tolerance\n",
    "\t\t\timage_aspect_ratio = img_obj['width'] / img_obj['height']\n",
    "\t\t\tif abs(image_aspect_ratio - 1) > image_aspect_ratio_tolerance:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Check if the image is already in the images list\n",
    "\t\t\tif any(img['image_id'] == img_obj['id'] for img in images):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Get the image url\n",
    "\t\t\timg_url = img_obj['coco_url']\n",
    "\t\t\t# Get the captions for the image\n",
    "\t\t\timg_captions = []\n",
    "\t\t\tcaptions = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tfor caption in captions:\n",
    "\t\t\t\tcaption_text = caption['caption'].strip()\n",
    "\t\t\t\tif len(caption_text) > 1:\n",
    "\t\t\t\t\timg_captions.append(caption_text)\n",
    "\t\t\t# Discard the image if the number of captions is less than the minimum\n",
    "\t\t\tif len(img_captions) < MIN_IMAGE_CAPTIONS:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Discard the image if it has no classes\n",
    "\t\t\tclasses = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tif len(classes) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Create a classes object with the fields: \"class_id\", \"class_name\", \"class_area\"\n",
    "\t\t\tclasses_obj = {}\n",
    "\t\t\tfor class_obj in classes:\n",
    "\t\t\t\tclass_id = class_obj['category_id']\n",
    "\t\t\t\tclass_name = coco_instances.cats[class_id]['name']\n",
    "\t\t\t\tclass_area = class_obj['area']\n",
    "\t\t\t\tif class_id not in classes_obj:\n",
    "\t\t\t\t\tclasses_obj[class_id] = {\n",
    "\t\t\t\t\t\t\"class_id\": class_id,\n",
    "\t\t\t\t\t\t\"class_name\": class_name,\n",
    "\t\t\t\t\t\t\"class_area\": class_area,\n",
    "\t\t\t\t\t\t\"class_count\": 1,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_area'] += class_area\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_count'] += 1\n",
    "\t\t\t# Convert the classes object to a list\n",
    "\t\t\tclasses_obj = list(classes_obj.values())\n",
    "\t\t\t# Sort the classes by area size\n",
    "\t\t\tclasses_obj = sorted(classes_obj, key=lambda x: x['class_area'], reverse=True)\n",
    "\t\t\t# Add the image to the images list\n",
    "\t\t\timages_list_object = {\n",
    "\t\t\t\t\"image_id\": img_obj['id'],\n",
    "\t\t\t\t\"image_url\": img_url,\n",
    "\t\t\t\t\"image_width\": img_obj['width'],\n",
    "\t\t\t\t\"image_height\": img_obj['height'],\n",
    "\t\t\t\t\"image_captions\": img_captions,\n",
    "\t\t\t\t\"image_classes\": classes_obj,\n",
    "\t\t\t\t\"image_data\": None # Will be filled later\n",
    "\t\t\t}\n",
    "\t\t\timages.append(images_list_object)\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif number_of_images >= 1 and len(images) >= number_of_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Return the images list\n",
    "\t\treturn images\n",
    "\tprint(\"Selecting images with a square aspect ratio...\")\n",
    "\t# Get the images that have a square aspect ratio first\n",
    "\timages = select_images_list(0)\n",
    "\t# Get the remaining images with the given aspect ratio tolerance\n",
    "\tif len(images) < number_of_images or number_of_images == -1:\n",
    "\t\tsquare_aspect_ratio_images = len(images)\n",
    "\t\tprint(\"> Found \" + str(square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio, looking for the remaining images...\")\n",
    "\t\tprint(\"Looking for remaining images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"% (either a \" + str(1 + MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio or a \" + str(1 - MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio)...\")\n",
    "\t\timages = select_images_list(MAX_ASPECT_RATIO_TOLERANCE)\n",
    "\t\tnon_square_aspect_ratio_images = len(images) - square_aspect_ratio_images\n",
    "\t\t# Print the number of images found\n",
    "\t\tprint(\"> Found \" + str(non_square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" )  + \" more images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"%.\")\n",
    "\telse:\n",
    "\t\tprint(\"> Found \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio.\")\n",
    "\t# Print a message based on the number of images found\n",
    "\tif len(images) < number_of_images and number_of_images != -1:\n",
    "\t\tprint(\"WARNING: Could not find enough images with the required aspect ratio tolerance, only \" + str(len(images)) + \" / \" + str(number_of_images) + \" images found.\")\n",
    "\telse:\n",
    "\t\tprint(\"DONE: Found all \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with the required aspect ratio tolerance.\")\n",
    "\t# Get all the image data\n",
    "\tif process_images:\n",
    "\t\tfor img in tqdm(images, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\t\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\t# Return the images list\n",
    "\treturn images\n",
    "\n",
    "# List of image objects used for the training of the Vision Transformer model\n",
    "images_db = []\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Check if the images list should be rebuilt or loaded\n",
    "\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\tif os.path.exists(images_db_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\t\twith open(images_db_file, 'r') as f:\n",
    "\t\t\timages_db = json.load(f)\n",
    "\t\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "\telse:\n",
    "\t\t# Initialize the images list\n",
    "\t\timages_db = get_images_db(-1, False)\n",
    "\t\t# Save the images list to a JSON file\n",
    "\t\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\t\tprint(\"Saving the images list to the file: \", images_db_file)\n",
    "\t\twith open(images_db_file, 'w') as f:\n",
    "\t\t\tjson.dump(images_db, f)\n",
    "\n",
    "\t# Print the final number of images in the dataset\n",
    "\tprint(\"\\nNumber of loaded images in the dataset: \" + str(len(images_db)) + (\"/\" + str(NUMBER_OF_IMAGES_IN_DB) if NUMBER_OF_IMAGES_IN_DB != -1 else \"\"))\n",
    "else:\n",
    "\t# Load the imaged_db.json file from the demo folder\n",
    "\timages_db_file = os.path.join(DEMO_FOLDER, \"images_db.json\")\n",
    "\tif os.path.exists(images_db_file):\n",
    "\t\twith open(images_db_file, 'r') as f:\n",
    "\t\t\timages_db = json.load(f)\n",
    "\t\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list from the file: \", images_db_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list from the file: \" + images_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the classes dictionary from the file:  src/demo/100/classes.json\n"
     ]
    }
   ],
   "source": [
    "# Create a \"classes\" dictionary with the classes found in the dataset and, for each of them, a list of the images in which they appear\n",
    "\n",
    "# initialize the classes dictionary\n",
    "classes = {}\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Function to get the classes dictionary from the images\n",
    "\tdef get_classes_dict():\n",
    "\t\t# Initialize the classes list\n",
    "\t\tclasses = {}\n",
    "\t\t# Get the classes from the images\n",
    "\t\tfor i in tqdm(range(len(images_db)), desc=\"Processing images for classes...\"):\n",
    "\t\t\timg = images_db[i]\n",
    "\t\t\tfor class_obj in img['image_classes']:\n",
    "\t\t\t\t# Get the class id\n",
    "\t\t\t\tclass_id = class_obj['class_id']\n",
    "\t\t\t\t# Add the class to the classes list if it does not exist\n",
    "\t\t\t\tif class_id not in classes.keys():\n",
    "\t\t\t\t\tclasses[class_id] = []\n",
    "\t\t\t\t# Add the image index to the class list\n",
    "\t\t\t\tclasses[class_id].append(i)\n",
    "\t\tprint(\"Created the classes list from the images with \" + str(len(classes)) + \" classes.\")\n",
    "\t\t# Discard the classes with less than the minimum number of images\n",
    "\t\tclasses = {k: v for k, v in classes.items() if len(v) >= MIN_IMAGES_PER_CLASS}\n",
    "\t\t# Sort classes by the number of images\n",
    "\t\tclasses = {k: v for k, v in sorted(classes.items(), key=lambda item: len(item[1]), reverse=True)}\n",
    "\t\tprint(\"Discarded the classes with less than \" + str(MIN_IMAGES_PER_CLASS) + \" images: \" + str(len(classes)) + \" / \" + str(len(classes.keys()) + len(classes)) + \" classes remaining.\")\n",
    "\t\t# Return the classes list\n",
    "\t\treturn classes\n",
    "\n",
    "\t# Get the classes dictionary if it already exists, otherwise create it\n",
    "\tclasses_file = os.path.join(DATA_FOLDER, \"classes.json\")\n",
    "\tif os.path.exists(classes_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\t\twith open(classes_file, 'r') as f:\n",
    "\t\t\tclasses = json.load(f)\n",
    "\t\tif len(classes) > 0:\n",
    "\t\t\tprint(\"Loaded the classes dictionary from the file: \", classes_file)\n",
    "\telse:\n",
    "\t\tprint(\"Creating the classes dictionary from the images...\")\n",
    "\t\tclasses = get_classes_dict()\n",
    "\t\t# Save the classes dictionary to a JSON file\n",
    "\t\tprint(\"Saving the classes dictionary to the file: \", classes_file)\n",
    "\t\twith open(classes_file, 'w') as f:\n",
    "\t\t\tjson.dump(classes, f)\n",
    "else:\n",
    "\t# Load the classes.json file from the demo folder\n",
    "\tclasses_file = os.path.join(DEMO_FOLDER, \"classes.json\")\n",
    "\tif os.path.exists(classes_file):\n",
    "\t\twith open(classes_file, 'r') as f:\n",
    "\t\t\tclasses = json.load(f)\n",
    "\t\tprint(\"Loaded the classes dictionary from the file: \", classes_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo classes dictionary from the file: \", classes_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo classes dictionary from the file: \" + classes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of loaded images in the dataset: 100\n",
      "Number of classes in the dataset: 77\n"
     ]
    }
   ],
   "source": [
    "# Update the images in the images DB to finally only include the images that have the classes in the classes list, with MIN_IMAGES_PER_CLASS images per class, and to populate the images list with the base64 encoding of the images\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Function to update the images list to only include the images that have the classes in the classes list\n",
    "\tdef update_images_db_based_on_classes(max_images):\n",
    "\t\t# Number of classes to maintain the designated number of images\n",
    "\t\tclasses_count = math.ceil(max_images / MIN_IMAGES_PER_CLASS)\n",
    "\t\t# Initialize the new images list\n",
    "\t\tnew_images_db = []\n",
    "\t\timages_db_ids_map = {}\n",
    "\t\t# Get the classes to maintain the designated number of images\n",
    "\t\tclasses_to_maintain = list(classes.keys())[:classes_count]\n",
    "\t\t# Create a new classes list with the classes found in the new images list\n",
    "\t\tnew_classes = {}\n",
    "\t\t# Get the images to maintain the designated number of images\n",
    "\t\tfor i in tqdm(range(len(images_db)), desc=\"Processing images for classes...\"):\n",
    "\t\t\timg = images_db[i]\n",
    "\t\t\t# Check if the image has any of the classes to maintain\n",
    "\t\t\tif any(class_obj['class_id'] in classes_to_maintain for class_obj in img['image_classes']):\n",
    "\t\t\t\tnew_images_db.append(img)\n",
    "\t\t\t\timages_db_ids_map[i] = len(new_images_db) - 1\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif len(new_images_db) >= max_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Get the classes from the classes to maintain\n",
    "\t\tfor class_id in classes_to_maintain:\n",
    "\t\t\t# Remove any image index that is not in the new images list\n",
    "\t\t\t# new_classes[class_id] = [i for i in classes[class_id] if i < len(new_images_db)]\n",
    "\t\t\tnew_classes[class_id] = [images_db_ids_map[i] for i in classes[class_id] if i in images_db_ids_map]\n",
    "\t\t# Sort the classes by the number of images\n",
    "\t\tnew_classes = { k: v for k, v in sorted(new_classes.items(), key=lambda item: len(item[1]), reverse=True) }\n",
    "\t\t# Return the new images list\n",
    "\t\treturn new_images_db, new_classes\n",
    "\n",
    "\t# Update the images list to only include the images that have the classes in the classes list\n",
    "\tmax_images = NUMBER_OF_IMAGES_IN_DB if NUMBER_OF_IMAGES_IN_DB != -1 else len(images_db)\n",
    "\tprint(\"\\nUpdating the images list to only include the \" + str(len(classes)) + \" classes with at least \" + str(MIN_IMAGES_PER_CLASS) + \" images, not exceeding \" + str(max_images) + \" images...\")\n",
    "\timages_db, classes = update_images_db_based_on_classes(max_images)\n",
    "\tprint(\"DONE: Updated the images list, now containing \" + str(len(images_db)) + \" images.\")\n",
    "\tprint(\"> Final number of classes in the dataset: \" + str(len(classes)))\n",
    "\n",
    "\t# Update the images list to include the base64 encoding of the images\n",
    "\tprint(\"Computing the BASE64 images encoding for the images list...\")\n",
    "\tfor img in tqdm(images_db, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\tprint(\"DONE: Computed the BASE64 images encoding for the images list.\")\n",
    "\n",
    "\t# Save the updated images list to a JSON file\n",
    "\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\tprint(\"Saving the updated images list to the file: \", images_db_file)\n",
    "\twith open(images_db_file, 'w') as f:\n",
    "\t\tjson.dump(images_db, f)\n",
    "\n",
    "# Print the final number of images in the dataset\n",
    "print(\"\\nNumber of loaded images in the dataset: \" + str(len(images_db)))\n",
    "print(\"Number of classes in the dataset: \" + str(len(classes)))\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRINT_EXAMPLES:\n",
    "\n",
    "\t# Print the first image object example\n",
    "\texample_image_index = -1\n",
    "\tprint(\"Image object example: \")\n",
    "\tutils.print_json(images_db[example_image_index], 2)\n",
    "\n",
    "\t# Print the actual image file\n",
    "\timage_b64_string = images_db[example_image_index]['image_data'] if images_db[example_image_index]['image_data'] != None else utils.get_image_data_as_base64(images_db[example_image_index]['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\timage = utils.get_image_from_b64_string(image_b64_string)\n",
    "\tprint(\"\\nActual image of the example (original size: \" + str(images_db[example_image_index]['image_width']) + \"x\" + str(images_db[example_image_index]['image_height']) + \" | downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(image)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Print how the Transformer model sees the image\n",
    "\tprint(\"\\nHow the Transformer model sees the image (downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "\t# Divide the image into smaller images representing the patches\n",
    "\timage_patches = []\n",
    "\tfor i in range(0, image.shape[0], IMAGE_PATCH_SIZE):\n",
    "\t\tfor j in range(0, image.shape[1], IMAGE_PATCH_SIZE):\n",
    "\t\t\timage_patch = image[i:i+IMAGE_PATCH_SIZE, j:j+IMAGE_PATCH_SIZE]\n",
    "\t\t\timage_patches.append(image_patch)\n",
    "\t# Display the image patches\n",
    "\tfig, axs = plt.subplots(IMAGE_PATCHES_PER_DIMENSION, IMAGE_PATCHES_PER_DIMENSION, figsize=(10, 10))\n",
    "\tfor i in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\t\tfor j in range(IMAGE_PATCHES_PER_DIMENSION):\n",
    "\t\t\taxs[i, j].imshow(image_patches[i*IMAGE_PATCHES_PER_DIMENSION+j])\n",
    "\t\t\taxs[i, j].axis('off')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes list sorted by number of images (77 classes out of ??? total MS COCO classes):\n",
      "    \"1\": [\n",
      "      0,\n",
      "      3,\n",
      "      6,\n",
      "      7,\n",
      "      8,\n",
      "      9,\n",
      "      13,\n",
      "      14,\n",
      "      17,\n",
      "      18,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 1109 elements)\"\n",
      "    ],\n",
      "    \"67\": [\n",
      "      19,\n",
      "      28,\n",
      "      29,\n",
      "      45,\n",
      "      47,\n",
      "      55,\n",
      "      56,\n",
      "      58,\n",
      "      59,\n",
      "      66,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 337 elements)\"\n",
      "    ],\n",
      "    \"47\": [\n",
      "      16,\n",
      "      23,\n",
      "      28,\n",
      "      46,\n",
      "      55,\n",
      "      56,\n",
      "      66,\n",
      "      67,\n",
      "      68,\n",
      "      69,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 235 elements)\"\n",
      "    ],\n",
      "    \"51\": [\n",
      "      11,\n",
      "      28,\n",
      "      29,\n",
      "      55,\n",
      "      67,\n",
      "      68,\n",
      "      69,\n",
      "      100,\n",
      "      101,\n",
      "      102,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 215 elements)\"\n",
      "    ],\n",
      "    \"62\": [\n",
      "      16,\n",
      "      28,\n",
      "      29,\n",
      "      40,\n",
      "      45,\n",
      "      53,\n",
      "      103,\n",
      "      104,\n",
      "      106,\n",
      "      124,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 195 elements)\"\n",
      "    ],\n",
      "    \"3\": [\n",
      "      0,\n",
      "      44,\n",
      "      45,\n",
      "      64,\n",
      "      85,\n",
      "      93,\n",
      "      96,\n",
      "      127,\n",
      "      133,\n",
      "      134,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 171 elements)\"\n",
      "    ],\n",
      "    \"44\": [\n",
      "      1,\n",
      "      23,\n",
      "      28,\n",
      "      55,\n",
      "      58,\n",
      "      66,\n",
      "      68,\n",
      "      69,\n",
      "      70,\n",
      "      101,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 148 elements)\"\n",
      "    ],\n",
      "    \"48\": [\n",
      "      23,\n",
      "      28,\n",
      "      29,\n",
      "      55,\n",
      "      102,\n",
      "      103,\n",
      "      115,\n",
      "      118,\n",
      "      122,\n",
      "      145,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 123 elements)\"\n",
      "    ],\n",
      "    \"50\": [\n",
      "      23,\n",
      "      28,\n",
      "      29,\n",
      "      100,\n",
      "      124,\n",
      "      126,\n",
      "      185,\n",
      "      207,\n",
      "      227,\n",
      "      293,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 113 elements)\"\n",
      "    ],\n",
      "    \"32\": [\n",
      "      39,\n",
      "      40,\n",
      "      42,\n",
      "      43,\n",
      "      84,\n",
      "      88,\n",
      "      140,\n",
      "      141,\n",
      "      218,\n",
      "      219,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 111 elements)\"\n",
      "    ],\n",
      "    \"49\": [\n",
      "      23,\n",
      "      28,\n",
      "      55,\n",
      "      67,\n",
      "      101,\n",
      "      104,\n",
      "      186,\n",
      "      227,\n",
      "      265,\n",
      "      270,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 110 elements)\"\n",
      "    ],\n",
      "    \"59\": [\n",
      "      18,\n",
      "      59,\n",
      "      60,\n",
      "      101,\n",
      "      107,\n",
      "      110,\n",
      "      111,\n",
      "      112,\n",
      "      113,\n",
      "      115,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 104 elements)\"\n",
      "    ],\n",
      "    \"31\": [\n",
      "      0,\n",
      "      7,\n",
      "      41,\n",
      "      71,\n",
      "      174,\n",
      "      209,\n",
      "      225,\n",
      "      252,\n",
      "      267,\n",
      "      272,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 103 elements)\"\n",
      "    ],\n",
      "    \"85\": [\n",
      "      7,\n",
      "      16,\n",
      "      26,\n",
      "      119,\n",
      "      120,\n",
      "      201,\n",
      "      202,\n",
      "      204,\n",
      "      206,\n",
      "      277,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 94 elements)\"\n",
      "    ],\n",
      "    \"77\": [\n",
      "      86,\n",
      "      95,\n",
      "      125,\n",
      "      129,\n",
      "      162,\n",
      "      203,\n",
      "      232,\n",
      "      276,\n",
      "      305,\n",
      "      332,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 93 elements)\"\n",
      "    ],\n",
      "    \"61\": [\n",
      "      56,\n",
      "      58,\n",
      "      98,\n",
      "      103,\n",
      "      104,\n",
      "      105,\n",
      "      186,\n",
      "      187,\n",
      "      265,\n",
      "      267,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 89 elements)\"\n",
      "    ],\n",
      "    \"8\": [\n",
      "      6,\n",
      "      36,\n",
      "      75,\n",
      "      85,\n",
      "      130,\n",
      "      134,\n",
      "      139,\n",
      "      169,\n",
      "      170,\n",
      "      196,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 88 elements)\"\n",
      "    ],\n",
      "    \"65\": [\n",
      "      16,\n",
      "      57,\n",
      "      61,\n",
      "      62,\n",
      "      84,\n",
      "      106,\n",
      "      109,\n",
      "      154,\n",
      "      163,\n",
      "      188,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 84 elements)\"\n",
      "    ],\n",
      "    \"27\": [\n",
      "      6,\n",
      "      46,\n",
      "      72,\n",
      "      94,\n",
      "      95,\n",
      "      128,\n",
      "      149,\n",
      "      174,\n",
      "      212,\n",
      "      225,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 82 elements)\"\n",
      "    ],\n",
      "    \"28\": [\n",
      "      7,\n",
      "      41,\n",
      "      45,\n",
      "      46,\n",
      "      196,\n",
      "      209,\n",
      "      252,\n",
      "      255,\n",
      "      257,\n",
      "      274,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 82 elements)\"\n",
      "    ],\n",
      "    \"18\": [\n",
      "      39,\n",
      "      90,\n",
      "      91,\n",
      "      95,\n",
      "      108,\n",
      "      144,\n",
      "      237,\n",
      "      245,\n",
      "      325,\n",
      "      334,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 82 elements)\"\n",
      "    ],\n",
      "    \"15\": [\n",
      "      14,\n",
      "      34,\n",
      "      45,\n",
      "      81,\n",
      "      87,\n",
      "      171,\n",
      "      175,\n",
      "      186,\n",
      "      196,\n",
      "      213,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 81 elements)\"\n",
      "    ],\n",
      "    \"56\": [\n",
      "      50,\n",
      "      51,\n",
      "      92,\n",
      "      118,\n",
      "      145,\n",
      "      181,\n",
      "      182,\n",
      "      183,\n",
      "      224,\n",
      "      263,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 76 elements)\"\n",
      "    ],\n",
      "    \"84\": [\n",
      "      22,\n",
      "      29,\n",
      "      39,\n",
      "      61,\n",
      "      81,\n",
      "      129,\n",
      "      165,\n",
      "      188,\n",
      "      205,\n",
      "      231,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 75 elements)\"\n",
      "    ],\n",
      "    \"64\": [\n",
      "      25,\n",
      "      29,\n",
      "      66,\n",
      "      121,\n",
      "      159,\n",
      "      167,\n",
      "      175,\n",
      "      186,\n",
      "      193,\n",
      "      232,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 72 elements)\"\n",
      "    ],\n",
      "    \"54\": [\n",
      "      14,\n",
      "      55,\n",
      "      100,\n",
      "      101,\n",
      "      102,\n",
      "      151,\n",
      "      226,\n",
      "      227,\n",
      "      228,\n",
      "      282,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 71 elements)\"\n",
      "    ],\n",
      "    \"41\": [\n",
      "      13,\n",
      "      52,\n",
      "      53,\n",
      "      54,\n",
      "      93,\n",
      "      147,\n",
      "      148,\n",
      "      150,\n",
      "      371,\n",
      "      513,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 69 elements)\"\n",
      "    ],\n",
      "    \"17\": [\n",
      "      36,\n",
      "      84,\n",
      "      86,\n",
      "      108,\n",
      "      127,\n",
      "      164,\n",
      "      175,\n",
      "      176,\n",
      "      213,\n",
      "      238,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 68 elements)\"\n",
      "    ],\n",
      "    \"52\": [\n",
      "      12,\n",
      "      29,\n",
      "      47,\n",
      "      48,\n",
      "      116,\n",
      "      125,\n",
      "      178,\n",
      "      179,\n",
      "      223,\n",
      "      293,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 63 elements)\"\n",
      "    ],\n",
      "    \"86\": [\n",
      "      16,\n",
      "      22,\n",
      "      24,\n",
      "      25,\n",
      "      121,\n",
      "      159,\n",
      "      161,\n",
      "      175,\n",
      "      186,\n",
      "      202,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 63 elements)\"\n",
      "    ],\n",
      "    \"4\": [\n",
      "      0,\n",
      "      1,\n",
      "      71,\n",
      "      72,\n",
      "      73,\n",
      "      74,\n",
      "      128,\n",
      "      136,\n",
      "      169,\n",
      "      237,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 61 elements)\"\n",
      "    ],\n",
      "    \"16\": [\n",
      "      4,\n",
      "      5,\n",
      "      35,\n",
      "      87,\n",
      "      123,\n",
      "      134,\n",
      "      173,\n",
      "      209,\n",
      "      247,\n",
      "      273,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 61 elements)\"\n",
      "    ],\n",
      "    \"46\": [\n",
      "      113,\n",
      "      183,\n",
      "      234,\n",
      "      236,\n",
      "      270,\n",
      "      280,\n",
      "      282,\n",
      "      331,\n",
      "      359,\n",
      "      367,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 60 elements)\"\n",
      "    ],\n",
      "    \"37\": [\n",
      "      9,\n",
      "      155,\n",
      "      160,\n",
      "      321,\n",
      "      323,\n",
      "      324,\n",
      "      390,\n",
      "      394,\n",
      "      430,\n",
      "      431,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 59 elements)\"\n",
      "    ],\n",
      "    \"42\": [\n",
      "      17,\n",
      "      229,\n",
      "      299,\n",
      "      357,\n",
      "      358,\n",
      "      362,\n",
      "      475,\n",
      "      619,\n",
      "      622,\n",
      "      713,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 59 elements)\"\n",
      "    ],\n",
      "    \"2\": [\n",
      "      52,\n",
      "      164,\n",
      "      165,\n",
      "      170,\n",
      "      195,\n",
      "      212,\n",
      "      241,\n",
      "      249,\n",
      "      262,\n",
      "      499,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 57 elements)\"\n",
      "    ],\n",
      "    \"81\": [\n",
      "      29,\n",
      "      124,\n",
      "      126,\n",
      "      239,\n",
      "      304,\n",
      "      333,\n",
      "      372,\n",
      "      395,\n",
      "      429,\n",
      "      446,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 55 elements)\"\n",
      "    ],\n",
      "    \"10\": [\n",
      "      6,\n",
      "      80,\n",
      "      134,\n",
      "      135,\n",
      "      167,\n",
      "      206,\n",
      "      212,\n",
      "      215,\n",
      "      225,\n",
      "      242,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 52 elements)\"\n",
      "    ],\n",
      "    \"73\": [\n",
      "      129,\n",
      "      154,\n",
      "      231,\n",
      "      271,\n",
      "      272,\n",
      "      332,\n",
      "      380,\n",
      "      434,\n",
      "      486,\n",
      "      504,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 51 elements)\"\n",
      "    ],\n",
      "    \"55\": [\n",
      "      10,\n",
      "      11,\n",
      "      29,\n",
      "      49,\n",
      "      51,\n",
      "      68,\n",
      "      179,\n",
      "      291,\n",
      "      292,\n",
      "      307,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 50 elements)\"\n",
      "    ],\n",
      "    \"60\": [\n",
      "      15,\n",
      "      97,\n",
      "      98,\n",
      "      99,\n",
      "      105,\n",
      "      153,\n",
      "      185,\n",
      "      265,\n",
      "      266,\n",
      "      296,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 50 elements)\"\n",
      "    ],\n",
      "    \"6\": [\n",
      "      7,\n",
      "      137,\n",
      "      169,\n",
      "      170,\n",
      "      198,\n",
      "      313,\n",
      "      348,\n",
      "      410,\n",
      "      411,\n",
      "      498,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 48 elements)\"\n",
      "    ],\n",
      "    \"79\": [\n",
      "      68,\n",
      "      69,\n",
      "      124,\n",
      "      235,\n",
      "      236,\n",
      "      275,\n",
      "      294,\n",
      "      295,\n",
      "      300,\n",
      "      306,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 48 elements)\"\n",
      "    ],\n",
      "    \"57\": [\n",
      "      28,\n",
      "      50,\n",
      "      66,\n",
      "      92,\n",
      "      181,\n",
      "      182,\n",
      "      263,\n",
      "      296,\n",
      "      319,\n",
      "      353,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 47 elements)\"\n",
      "    ],\n",
      "    \"63\": [\n",
      "      22,\n",
      "      108,\n",
      "      159,\n",
      "      193,\n",
      "      267,\n",
      "      271,\n",
      "      328,\n",
      "      473,\n",
      "      485,\n",
      "      523,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 46 elements)\"\n",
      "    ],\n",
      "    \"7\": [\n",
      "      138,\n",
      "      174,\n",
      "      216,\n",
      "      250,\n",
      "      251,\n",
      "      287,\n",
      "      288,\n",
      "      314,\n",
      "      342,\n",
      "      414,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 45 elements)\"\n",
      "    ],\n",
      "    \"33\": [\n",
      "      44,\n",
      "      129,\n",
      "      143,\n",
      "      174,\n",
      "      176,\n",
      "      256,\n",
      "      272,\n",
      "      337,\n",
      "      379,\n",
      "      505,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 44 elements)\"\n",
      "    ],\n",
      "    \"70\": [\n",
      "      70,\n",
      "      106,\n",
      "      117,\n",
      "      166,\n",
      "      238,\n",
      "      239,\n",
      "      283,\n",
      "      333,\n",
      "      341,\n",
      "      372,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 42 elements)\"\n",
      "    ],\n",
      "    \"19\": [\n",
      "      222,\n",
      "      381,\n",
      "      422,\n",
      "      605,\n",
      "      784,\n",
      "      825,\n",
      "      859,\n",
      "      921,\n",
      "      994,\n",
      "      995,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 42 elements)\"\n",
      "    ],\n",
      "    \"53\": [\n",
      "      68,\n",
      "      146,\n",
      "      178,\n",
      "      225,\n",
      "      260,\n",
      "      262,\n",
      "      370,\n",
      "      386,\n",
      "      426,\n",
      "      446,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 41 elements)\"\n",
      "    ],\n",
      "    \"5\": [\n",
      "      30,\n",
      "      31,\n",
      "      32,\n",
      "      33,\n",
      "      75,\n",
      "      76,\n",
      "      77,\n",
      "      78,\n",
      "      79,\n",
      "      130,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 38 elements)\"\n",
      "    ],\n",
      "    \"43\": [\n",
      "      20,\n",
      "      155,\n",
      "      160,\n",
      "      192,\n",
      "      390,\n",
      "      394,\n",
      "      661,\n",
      "      720,\n",
      "      721,\n",
      "      754,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 37 elements)\"\n",
      "    ],\n",
      "    \"88\": [\n",
      "      27,\n",
      "      50,\n",
      "      61,\n",
      "      116,\n",
      "      205,\n",
      "      233,\n",
      "      269,\n",
      "      308,\n",
      "      328,\n",
      "      341,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 37 elements)\"\n",
      "    ],\n",
      "    \"72\": [\n",
      "      22,\n",
      "      106,\n",
      "      231,\n",
      "      248,\n",
      "      268,\n",
      "      272,\n",
      "      505,\n",
      "      557,\n",
      "      595,\n",
      "      599,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 36 elements)\"\n",
      "    ],\n",
      "    \"9\": [\n",
      "      35,\n",
      "      87,\n",
      "      124,\n",
      "      142,\n",
      "      144,\n",
      "      253,\n",
      "      284,\n",
      "      377,\n",
      "      378,\n",
      "      417,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 36 elements)\"\n",
      "    ],\n",
      "    \"25\": [\n",
      "      2,\n",
      "      3,\n",
      "      132,\n",
      "      168,\n",
      "      214,\n",
      "      285,\n",
      "      374,\n",
      "      451,\n",
      "      500,\n",
      "      692,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 34 elements)\"\n",
      "    ],\n",
      "    \"11\": [\n",
      "      80,\n",
      "      93,\n",
      "      136,\n",
      "      217,\n",
      "      257,\n",
      "      450,\n",
      "      452,\n",
      "      666,\n",
      "      693,\n",
      "      694,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 34 elements)\"\n",
      "    ],\n",
      "    \"75\": [\n",
      "      22,\n",
      "      111,\n",
      "      158,\n",
      "      163,\n",
      "      193,\n",
      "      557,\n",
      "      680,\n",
      "      783,\n",
      "      801,\n",
      "      829,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 33 elements)\"\n",
      "    ],\n",
      "    \"58\": [\n",
      "      67,\n",
      "      151,\n",
      "      152,\n",
      "      208,\n",
      "      209,\n",
      "      354,\n",
      "      355,\n",
      "      514,\n",
      "      530,\n",
      "      581,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 33 elements)\"\n",
      "    ],\n",
      "    \"35\": [\n",
      "      94,\n",
      "      95,\n",
      "      149,\n",
      "      184,\n",
      "      261,\n",
      "      317,\n",
      "      318,\n",
      "      351,\n",
      "      352,\n",
      "      382,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 33 elements)\"\n",
      "    ],\n",
      "    \"38\": [\n",
      "      64,\n",
      "      65,\n",
      "      194,\n",
      "      195,\n",
      "      196,\n",
      "      197,\n",
      "      198,\n",
      "      199,\n",
      "      200,\n",
      "      273,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 32 elements)\"\n",
      "    ],\n",
      "    \"34\": [\n",
      "      90,\n",
      "      144,\n",
      "      290,\n",
      "      507,\n",
      "      508,\n",
      "      542,\n",
      "      543,\n",
      "      602,\n",
      "      603,\n",
      "      604,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 30 elements)\"\n",
      "    ],\n",
      "    \"90\": [\n",
      "      21,\n",
      "      83,\n",
      "      114,\n",
      "      126,\n",
      "      157,\n",
      "      254,\n",
      "      303,\n",
      "      304,\n",
      "      327,\n",
      "      361,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 28 elements)\"\n",
      "    ],\n",
      "    \"82\": [\n",
      "      124,\n",
      "      300,\n",
      "      364,\n",
      "      446,\n",
      "      564,\n",
      "      624,\n",
      "      626,\n",
      "      731,\n",
      "      732,\n",
      "      733,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 26 elements)\"\n",
      "    ],\n",
      "    \"39\": [\n",
      "      437,\n",
      "      438,\n",
      "      837,\n",
      "      848,\n",
      "      965,\n",
      "      997,\n",
      "      1009,\n",
      "      1323,\n",
      "      1439,\n",
      "      1503,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 24 elements)\"\n",
      "    ],\n",
      "    \"40\": [\n",
      "      8,\n",
      "      9,\n",
      "      259,\n",
      "      696,\n",
      "      965,\n",
      "      997,\n",
      "      1043,\n",
      "      1439,\n",
      "      1503,\n",
      "      1504,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 23 elements)\"\n",
      "    ],\n",
      "    \"36\": [\n",
      "      94,\n",
      "      95,\n",
      "      96,\n",
      "      264,\n",
      "      351,\n",
      "      352,\n",
      "      428,\n",
      "      613,\n",
      "      957,\n",
      "      1299,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 21 elements)\"\n",
      "    ],\n",
      "    \"22\": [\n",
      "      220,\n",
      "      221,\n",
      "      316,\n",
      "      600,\n",
      "      989,\n",
      "      1038,\n",
      "      1076,\n",
      "      1195,\n",
      "      1239,\n",
      "      1446,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 20 elements)\"\n",
      "    ],\n",
      "    \"21\": [\n",
      "      37,\n",
      "      38,\n",
      "      456,\n",
      "      459,\n",
      "      460,\n",
      "      746,\n",
      "      770,\n",
      "      815,\n",
      "      915,\n",
      "      917,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 19 elements)\"\n",
      "    ],\n",
      "    \"74\": [\n",
      "      272,\n",
      "      380,\n",
      "      486,\n",
      "      614,\n",
      "      964,\n",
      "      1061,\n",
      "      1085,\n",
      "      1127,\n",
      "      1154,\n",
      "      1305,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 19 elements)\"\n",
      "    ],\n",
      "    \"13\": [\n",
      "      249,\n",
      "      501,\n",
      "      950,\n",
      "      986,\n",
      "      991,\n",
      "      1028,\n",
      "      1071,\n",
      "      1272,\n",
      "      1440,\n",
      "      1441,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 18 elements)\"\n",
      "    ],\n",
      "    \"24\": [\n",
      "      421,\n",
      "      451,\n",
      "      463,\n",
      "      824,\n",
      "      1200,\n",
      "      1242,\n",
      "      1518,\n",
      "      1535,\n",
      "      1551,\n",
      "      1580,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 18 elements)\"\n",
      "    ],\n",
      "    \"20\": [\n",
      "      82,\n",
      "      172,\n",
      "      245,\n",
      "      454,\n",
      "      984,\n",
      "      1177,\n",
      "      1261,\n",
      "      1389,\n",
      "      1533,\n",
      "      1550,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 15 elements)\"\n",
      "    ],\n",
      "    \"76\": [\n",
      "      248,\n",
      "      462,\n",
      "      515,\n",
      "      560,\n",
      "      611,\n",
      "      614,\n",
      "      829,\n",
      "      964,\n",
      "      1033,\n",
      "      1127,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 15 elements)\"\n",
      "    ],\n",
      "    \"87\": [\n",
      "      303,\n",
      "      532,\n",
      "      902,\n",
      "      1132,\n",
      "      1185,\n",
      "      1217,\n",
      "      1634,\n",
      "      1674,\n",
      "      1690,\n",
      "      1916,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 15 elements)\"\n",
      "    ],\n",
      "    \"23\": [\n",
      "      89,\n",
      "      177,\n",
      "      258,\n",
      "      781,\n",
      "      1079,\n",
      "      1116,\n",
      "      1280,\n",
      "      1565,\n",
      "      1566,\n",
      "      1567,\n",
      "      \"...\",\n",
      "      \"(Truncated to 10 out of 14 elements)\"\n",
      "    ],\n",
      "    \"78\": [\n",
      "      124,\n",
      "      275,\n",
      "      300,\n",
      "      306,\n",
      "      446,\n",
      "      690,\n",
      "      734,\n",
      "      1069,\n",
      "      1604,\n",
      "      1875\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "# Print the classes list\n",
    "total_classes = -1 if USE_DEMO_DATA else len(coco_instances.cats)\n",
    "print(\"\\nClasses list sorted by number of images (\" + str(len(classes)) + \" classes out of \" + (str(total_classes) if total_classes != -1 else \"???\") + \" total MS COCO classes):\")\n",
    "utils.print_json(classes, 2, truncate_large_lists=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images list for the indexing dataset from the file:  src/demo/100/images_db_indexing.json\n",
      "Loaded the images list for the image retrieval dataset from the file:  src/demo/100/images_db_image_retrieval.json\n",
      "\n",
      "Number of images in the indexing dataset: 30\n",
      "Image IDs max length: 2\n"
     ]
    }
   ],
   "source": [
    "# Split the images list into a list for the indexing dataset (i.e. images in the database) and a list for the image retrieval dataset (i.e. similar images to retrieve images in the DB)\n",
    "\n",
    "# List of images for the indexing and image retrieval datasets\n",
    "images_db_indexing = []\t# List of images for the indexing dataset\n",
    "images_db_image_retrieval = {} # Dictionary containing image IDs of images NOT in the indexing dataset as keys and the list of similar images in the indexing dataset as values\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Create the indexing and image retrieval datasets from the images list\n",
    "\tfor class_id in classes.keys():\n",
    "\t\tclass_obj = classes[class_id]\n",
    "\t\tindexing_number = int(len(class_obj) * (1 - IMAGE_RETRIEVAL_DB_PERCENTAGE))\n",
    "\t\tsimilar_images = []\n",
    "\t\tremap_image_ids = {}\n",
    "\t\tfor i in range(len(class_obj)):\n",
    "\t\t\tis_in_db = i < indexing_number\n",
    "\t\t\timg_id = class_obj[i]\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg = images_db[img_id]\n",
    "\t\t\tif is_in_db:\n",
    "\t\t\t\t# Add the image to the indexing dataset\n",
    "\t\t\t\timages_db_indexing.append(img)\n",
    "\t\t\t\t# Add the image to the similar images list\n",
    "\t\t\t\tsimilar_images.append(img_id)\n",
    "\t\t\t\t# Store the remapping of the image IDs\n",
    "\t\t\t\tremap_image_ids[img_id] = len(images_db_indexing) - 1\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Add the image to the image retrieval dataset\n",
    "\t\t\t\timages_db_image_retrieval[img_id] = [ remap_image_ids[i] for i in similar_images ]\n",
    "\t# Save the indexing and image retrieval datasets to JSON files\n",
    "\timages_db_indexing_file = os.path.join(DATA_FOLDER, \"images_db_indexing.json\")\n",
    "\tprint(\"Saving the images list for the indexing dataset to the file: \", images_db_indexing_file)\n",
    "\twith open(images_db_indexing_file, 'w') as f:\n",
    "\t\tjson.dump(images_db_indexing, f)\n",
    "\timages_db_image_retrieval_file = os.path.join(DATA_FOLDER, \"images_db_image_retrieval.json\")\n",
    "\tprint(\"Saving the images list for the image retrieval dataset to the file: \", images_db_image_retrieval_file)\n",
    "\twith open(images_db_image_retrieval_file, 'w') as f:\n",
    "\t\tjson.dump(images_db_image_retrieval, f)\n",
    "else:\n",
    "\t# Load the images_db_indexing.json and images_db_image_retrieval.json files from the demo folder\n",
    "\timages_db_indexing_file = os.path.join(DEMO_FOLDER, \"images_db_indexing.json\")\n",
    "\timages_db_image_retrieval_file = os.path.join(DEMO_FOLDER, \"images_db_image_retrieval.json\")\n",
    "\tif os.path.exists(images_db_indexing_file):\n",
    "\t\twith open(images_db_indexing_file, 'r') as f:\n",
    "\t\t\timages_db_indexing = json.load(f)\n",
    "\t\tprint(\"Loaded the images list for the indexing dataset from the file: \", images_db_indexing_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list for the indexing dataset from the file: \", images_db_indexing_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list for the indexing dataset from the file: \" + images_db_indexing_file)\n",
    "\tif os.path.exists(images_db_image_retrieval_file):\n",
    "\t\twith open(images_db_image_retrieval_file, 'r') as f:\n",
    "\t\t\timages_db_image_retrieval = json.load(f)\n",
    "\t\tprint(\"Loaded the images list for the image retrieval dataset from the file: \", images_db_image_retrieval_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list for the image retrieval dataset from the file: \", images_db_image_retrieval_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list for the image retrieval dataset from the file: \" + images_db_image_retrieval_file)\n",
    "\n",
    "# Compute the max length of the image IDS (we consider the index of the image in the \"images_db\" as the image ID)\n",
    "max_image_id_length = len(str(len(images_db_indexing)))\n",
    "# Number of output tokens for the encoded image IDs (the 10 digits [0-9] plus the 3 special tokens, i.e. end of sequence, padding, start of sequence)\n",
    "output_tokens = 10 + 3\n",
    "\n",
    "# Print the final number of images in the datasets\n",
    "print(\"\\nNumber of images in the indexing dataset: \" + str(len(images_db_indexing)))\n",
    "print(\"Image IDs max length: \" + str(max_image_id_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Vision Transformer Indexing Dataset from src/demo/100/transformer_indexing_dataset.json...\n",
      "Loaded 30 images from src/demo/100/transformer_indexing_dataset.json\n",
      "Loading the Vision Transformer Indexing Dataset from src/demo/100/transformer_image_retrieval_dataset.json...\n",
      "Loaded 514 images from src/demo/100/transformer_image_retrieval_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Paths of the file in which the PyTorch datasets will be stored or from which they will be loaded\n",
    "transformer_indexing_dataset_file = (DATA_FOLDER + \"/\" if not USE_DEMO_DATA else DEMO_FOLDER) + \"transformer_indexing_dataset.json\"\n",
    "transformer_image_retrieval_dataset_file = (DATA_FOLDER + \"/\" if not USE_DEMO_DATA else DEMO_FOLDER) + \"transformer_image_retrieval_dataset.json\"\n",
    "\n",
    "# Build the Transformer Indexing Database for training the vision transformer\n",
    "transformer_indexing_dataset = datasets.TransformerIndexingDataset(\n",
    "\timages=images_db_indexing,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=IMAGE_PATCHES_PER_DIMENSION,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=transformer_indexing_dataset_file,\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION and not USE_DEMO_DATA\n",
    ")\n",
    "\n",
    "# Build the Transformer Image Retrieval Database for training the vision transformer\n",
    "transformer_image_retrieval_dataset = datasets.TransformerImageRetrievalDataset(\n",
    "\tall_images=images_db,\n",
    "\tsimilar_images=images_db_image_retrieval,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=IMAGE_PATCHES_PER_DIMENSION,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=transformer_image_retrieval_dataset_file,\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION and not USE_DEMO_DATA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRINT_EXAMPLES or True:\n",
    "\t# Print the first example from the Transformer Indexing Dataset\n",
    "\texample_index = random.randint(0, len(transformer_indexing_dataset)-1)\n",
    "\tprint(\"Example from the Transformer Indexing Dataset:\")\n",
    "\tprint(\"<encoded_image, encoded_image_id> tuple:\")\n",
    "\tprint(transformer_indexing_dataset[example_index])\n",
    "\n",
    "\t# Print the first example from the Transformer Image Retrieval Dataset\n",
    "\texample_index = random.randint(0, len(transformer_image_retrieval_dataset)-1)\n",
    "\tprint(\"\\nExample from the Transformer Image Retrieval Dataset:\")\n",
    "\tprint(\"<encoded_image, encoded_similar_image_id> tuple:\")\n",
    "\tprint(transformer_image_retrieval_dataset[example_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_EMBEDDINGS_SIZE = 128\n",
    "\n",
    "TRANSFORMER_INDEXING_TRAINING_EPOCHS = 250\n",
    "TRANSFORMER_RETRIEVAL_TRAINING_EPOCHS = 150\n",
    "\n",
    "def train_and_evaluate_transformer():\n",
    "\t''' Auxiliary function to train (or load checkpoints), show training results, and evaluate the transformer model of the given type '''\n",
    "\t\n",
    "\tdsi_transformer_args = {\n",
    "\t\t# Dimensionality of the input feature vectors to the Transformer (i.e. the size of the embeddings)\n",
    "\t\t\"embed_dim\": TRANSFORMER_EMBEDDINGS_SIZE, \n",
    "\t\t# Dimensionality of the hidden layer in the feed-forward networks within the Transformer\n",
    "\t\t\"hidden_dim\": 256, \n",
    "\t\t# Number of channels of the input images (e.g. 3 for RGB, 1 for grayscale, ecc...)\n",
    "\t\t\"num_channels\": 3,\t\n",
    "\t\t# Number of heads to use in the Multi-Head Attention block\n",
    "\t\t\"num_heads\": 4,\t\n",
    "\t\t# Number of layers to use in the Transformer\n",
    "\t\t\"num_layers\": 3,\n",
    "\t\t# Size of each batch\n",
    "\t\t\"batch_size\": 32,\n",
    "\t\t# Number of classes to predict (in my case, since I give an image with, concatenated, the N digits of the image ID, the num_classes is the number of possible digits of the image IDs, hence 10+3, including the special tokens)\n",
    "\t\t\"num_classes\": output_tokens,\n",
    "\t\t# Size of the image patches\n",
    "\t\t\"patch_size\": IMAGE_PATCH_SIZE,\n",
    "\t\t# Maximum number of patches an image can have\n",
    "\t\t\"num_patches\": IMAGE_PATCHES_PER_DIMENSION * IMAGE_PATCHES_PER_DIMENSION,\n",
    "\t\t# Maximum length of the image IDs\n",
    "\t\t\"img_id_max_length\": max_image_id_length,\n",
    "\t\t# Special tokens for the image IDs\n",
    "\t\t\"img_id_start_token\": 10,\n",
    "\t\t\"img_id_end_token\": 12,\n",
    "\t\t\"img_id_padding_token\": 11,\n",
    "\t\t# Dropout to apply in the feed-forward network and on the input encoding\n",
    "\t\t\"dropout\": 0.2,\n",
    "\t\t# Learning rate for the optimizer\n",
    "\t\t\"learning_rate\": 0.001,\n",
    "\t}\n",
    "\n",
    "\t# Initialize transformer model (using scheduled sampling)\n",
    "\ttransformer_model = models.DSI_VisionTransformer(**dsi_transformer_args)\n",
    "\n",
    "\t# Model's type string\n",
    "\tmodel_type_string = \"DSI_VisionTransformer\"\n",
    "\n",
    "\t# Model's checkpoint file\n",
    "\tmodel_checkpoint_file = MODELS_FOLDER + \"/\" + model_type_string + \"_\" + MODEL_CHECKPOINT_FILE\n",
    "\n",
    "\t# Train the model or load its saved checkpoint\n",
    "\ttransformer_retrieval_test_set = None\n",
    "\ttransformer_retrieval_test_set_file = DATA_FOLDER + f\"/{model_type_string}_transformer_retrieval_test_set.json\"\n",
    "\tif LOAD_MODELS_CHECKPOINTS and os.path.exists(model_checkpoint_file):\n",
    "\t\t# Load the saved models checkpoint\n",
    "\t\tprint(\"A checkpoint for the model exist, loading the saved model checkpoint...\")\n",
    "\t\ttransformer_model = models.DSI_VisionTransformer.load_from_checkpoint(model_checkpoint_file, **dsi_transformer_args)\n",
    "\t\tprint(\"Model checkpoint loaded.\")\n",
    "\t\t# Load the transformer retrieval test set from the JSON file\n",
    "\t\tprint(\"Loading the transformer retrieval test set from the JSON file...\")\n",
    "\t\twith open(transformer_retrieval_test_set_file, \"r\") as transformer_retrieval_test_set_file:\n",
    "\t\t\ttransformer_retrieval_test_set = json.load(transformer_retrieval_test_set_file)\n",
    "\t\tprint(\"Transformer retrieval test set loaded.\")\n",
    "\telse:\n",
    "\t\t# Create 2 loggers for the transformer model (one for the indexing task and one for the retrieval task)\n",
    "\t\ttransformer_loggers = None\n",
    "\t\tif wandb_api is not None:\n",
    "\t\t\ttransformer_wandb_logger_indexing = WandbLogger(log_model=\"all\", project=wandb_project, name=model_type_string + \" (Indexing)\")\n",
    "\t\t\ttransformer_wandb_logger_retrieval = WandbLogger(log_model=\"all\", project=wandb_project, name=model_type_string + \" (Retrieval)\")\n",
    "\t\t\ttransformer_loggers = [transformer_wandb_logger_indexing, transformer_wandb_logger_retrieval]\n",
    "\t\t# Train the transformer model (with scheduled sampling) for the indexing task\n",
    "\t\ttransformer_training_infos = training.train_transformer(\n",
    "\t\t\ttransformer_indexing_dataset=transformer_indexing_dataset,\n",
    "\t\t\ttransformer_retrieval_dataset=transformer_image_retrieval_dataset,\n",
    "\t\t\ttransformer_model=transformer_model,\n",
    "\t\t\tmax_epochs_list=[TRANSFORMER_INDEXING_TRAINING_EPOCHS, TRANSFORMER_RETRIEVAL_TRAINING_EPOCHS],\n",
    "\t\t\tbatch_size=transformer_model.hparams.batch_size,\n",
    "\t\t\tindexing_split_ratios=(1.0, 0.0),\n",
    "\t\t\tretrieval_split_ratios=(0.9, 0.05, 0.05),\n",
    "\t\t\tlogger=transformer_loggers,\n",
    "\t\t\tsave_path=model_checkpoint_file\n",
    "\t\t)\n",
    "\t\t# Show the wandb training run's dashboard\n",
    "\t\tif wandb_api is not None:\n",
    "\t\t\tindexing_run_id = transformer_training_infos[\"run_ids\"][\"indexing\"]\n",
    "\t\t\tif indexing_run_id is not None:\n",
    "\t\t\t\tprint(f\"Indexing training results for the {model_type_string} model:\")\n",
    "\t\t\t\tindexing_run_object: wandb_run.Run = wandb_api.run(f\"{wandb_entity}/{wandb_project}/{indexing_run_id}\")\n",
    "\t\t\t\tindexing_run_object.display(height=1000)\n",
    "\t\t\tretrieval_run_id = transformer_training_infos[\"run_ids\"][\"retrieval\"]\n",
    "\t\t\tif retrieval_run_id is not None:\n",
    "\t\t\t\tprint(f\"Retrieval training results for the {model_type_string} model:\")\n",
    "\t\t\t\tretrieval_run_object: wandb_run.Run = wandb_api.run(f\"{wandb_entity}/{wandb_project}/{retrieval_run_id}\")\n",
    "\t\t\t\tretrieval_run_object.display(height=1000)\n",
    "\t\t# Save the generated transformer retrieval test set to the JSON file\n",
    "\t\tprint(\"Saving the transformer retrieval test set to the JSON file...\")\n",
    "\t\tretrieval_test_dataset = transformer_training_infos[\"retrieval\"][\"test\"]\n",
    "\t\ttransformer_retrieval_test_set = {\n",
    "\t\t\t\"encoded_queries\": [],\n",
    "\t\t\t\"encoded_doc_ids\": []\n",
    "\t\t}\n",
    "\t\tretrieval_test_dataset_length = retrieval_test_dataset.__len__()\n",
    "\t\tfor i in range(retrieval_test_dataset_length):\n",
    "\t\t\tencoded_query, doc_id = retrieval_test_dataset.__getitem__(i)\n",
    "\t\t\ttransformer_retrieval_test_set[\"encoded_queries\"].append(encoded_query.tolist())\n",
    "\t\t\ttransformer_retrieval_test_set[\"encoded_doc_ids\"].append(doc_id.tolist())\n",
    "\t\twith open(transformer_retrieval_test_set_file, \"w\") as transformer_retrieval_test_set_file:\n",
    "\t\t\tjson.dump(transformer_retrieval_test_set, transformer_retrieval_test_set_file)\n",
    "\n",
    "\t'''\n",
    "\t# Evaluate the transformer model (for the retrieval task)\n",
    "\tif EVALUATE_MODELS:\n",
    "\t\ttransformer_retrieval_map_k = evaluation.compute_mean_average_precision_at_k(\n",
    "\t\t\tMODEL_TYPES.DSI_TRANSFORMER, queries_dict, docs_dict,\n",
    "\t\t\tk_documents=MAP_K, n_queries=MAP_N,\n",
    "\t\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\t\t# Keyword arguments for the Transformer model\n",
    "\t\t\tmodel=transformer_model, retrieval_dataset=transformer_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t\t)\n",
    "\t\ttransformer_retrieval_recall_k = evaluation.compute_recall_at_k(\n",
    "\t\t\tMODEL_TYPES.DSI_TRANSFORMER, queries_dict, docs_dict,\n",
    "\t\t\tk_documents=RECALL_K,\n",
    "\t\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\t\t# Keyword arguments for the Transformer model\n",
    "\t\t\tmodel=transformer_model, retrieval_dataset=transformer_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t\t)\n",
    "\t\tprint_model_evaluation_results(transformer_retrieval_map_k, transformer_retrieval_recall_k)\n",
    "\t''' \n",
    "\n",
    "\t# return transformer_model, transformer_retrieval_map_k, transformer_retrieval_recall_k\n",
    "\treturn transformer_model, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_folder: src/models/\n",
      "checkpoint_name: DSI_VisionTransformer_transformer.ckpt\n",
      "Training the model for the indexing task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | DSI_ViT | 512 K \n",
      "----------------------------------\n",
      "512 K     Trainable params\n",
      "0         Non-trainable params\n",
      "512 K     Total params\n",
      "2.050     Total estimated model params size (MB)\n",
      "C:\\Users\\valer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\valer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\valer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\valer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\utilities\\data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=30, C=3, H=160, W=160, N=2\n",
      "input.shape: torch.Size([30, 3, 160, 160])\n",
      "target.shape: torch.Size([30, 6])\n",
      "output.shape: torch.Size([30, 1, 13])\n",
      "generated_target.shape: torch.Size([30, 1])\n",
      "input.shape (i=1):torch.Size([30, 3, 160, 160])\n",
      "current_target.shape (i=1):torch.Size([30, 1])\n",
      "ids.shape: torch.Size([30, 1])\n",
      "M=1, N=2\n",
      "imgs.shape (processed): torch.Size([30, 100, 128])\n",
      "ids.shape (processed)): torch.Size([30, 1, 128])\n",
      "x.shape (1): torch.Size([30, 101, 128])\n",
      "x.shape (2): torch.Size([30, 102, 128])\n",
      "padding_mask.shape: torch.Size([30, 102])\n",
      "input.shape: torch.Size([102, 30, 128])\n",
      "padding_mask.shape: torch.Size([30, 102])\n",
      "attention_mask.shape: torch.Size([102, 102])\n",
      "input.shape: torch.Size([102, 30, 128])\n",
      "padding_mask.shape: torch.Size([30, 102])\n",
      "attention_mask.shape: torch.Size([102, 102])\n",
      "input.shape: torch.Size([102, 30, 128])\n",
      "padding_mask.shape: torch.Size([30, 102])\n",
      "attention_mask.shape: torch.Size([102, 102])\n",
      "x.shape (3): torch.Size([30, 102, 128])\n",
      "cls.shape: torch.Size([30, 128])\n",
      "out.shape: torch.Size([30, 13])\n",
      "new_output.shape: torch.Size([30, 13])\n",
      "output[:, i - 1].shape: torch.Size([30, 13])\n",
      "target.shape: torch.Size([30, 6])\n",
      "target_out.shape: torch.Size([30, 5])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (30) to match target batch_size (150).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate the vision transformer model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m teacher_forcing_transformer, teacher_forcing_transformer_map_k, teacher_forcing_transformer_recall_k \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 70\u001b[0m, in \u001b[0;36mtrain_and_evaluate_transformer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \ttransformer_loggers \u001b[38;5;241m=\u001b[39m [transformer_wandb_logger_indexing, transformer_wandb_logger_retrieval]\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Train the transformer model (with scheduled sampling) for the indexing task\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m transformer_training_infos \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtransformer_indexing_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_indexing_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtransformer_retrieval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_image_retrieval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_epochs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTRANSFORMER_INDEXING_TRAINING_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRANSFORMER_RETRIEVAL_TRAINING_EPOCHS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mindexing_split_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mretrieval_split_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m\t\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint_file\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Show the wandb training run's dashboard\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\valer\\Desktop\\computer_vision_project_dev\\src\\scripts\\training.py:112\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[1;34m(transformer_indexing_dataset, transformer_retrieval_dataset, transformer_model, max_epochs_list, batch_size, indexing_split_ratios, retrieval_split_ratios, logger, save_path, train_retrieval)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model for the indexing task...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m    101\u001b[0m \t\u001b[38;5;66;03m# Set the maximum number of epochs\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \tmax_epochs\u001b[38;5;241m=\u001b[39mmax_epochs_list[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m \tenable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    111\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing_validation_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained the model for the indexing task.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m indexing_run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         closure()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\core\\module.py:1303\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1266\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \n\u001b[0;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\core\\optimizer.py:152\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\adamw.py:164\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 164\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    167\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:318\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\valer\\Desktop\\computer_vision_project_dev\\src\\scripts\\models.py:85\u001b[0m, in \u001b[0;36mDSI_VisionTransformer.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m     83\u001b[0m \t\u001b[38;5;66;03m# print(\"batch:\", batch)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \t\u001b[38;5;66;03m# Training step for the model (compute the loss and accuracy)\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \tloss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \t\u001b[38;5;66;03m# Append the loss to the training losses list (for logging)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "File \u001b[1;32mc:\\Users\\valer\\Desktop\\computer_vision_project_dev\\src\\scripts\\models.py:385\u001b[0m, in \u001b[0;36mDSI_ViT.step\u001b[1;34m(self, batch, use_autoregression)\u001b[0m\n\u001b[0;32m    383\u001b[0m reshaped_output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\t\u001b[38;5;66;03m# Reshape the output tensor to have a shape of [B*N, num_classes]\u001b[39;00m\n\u001b[0;32m    384\u001b[0m reshaped_target_out \u001b[38;5;241m=\u001b[39m target_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\t\t\u001b[38;5;66;03m# Reshape the target_out tensor to have a shape of [B*N]\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshaped_target_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# Get the best prediction (to compute the accuracy) for the next token of the target sequence (i.e. the generated image ID token/digit)\u001b[39;00m\n\u001b[0;32m    387\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (30) to match target batch_size (150)."
     ]
    }
   ],
   "source": [
    "# Train and evaluate the vision transformer model\n",
    "teacher_forcing_transformer, teacher_forcing_transformer_map_k, teacher_forcing_transformer_recall_k = train_and_evaluate_transformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
